# services.py
import os
import io
import base64
import tempfile
import numpy as np
import math
import librosa
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation, FFMpegWriter
from matplotlib.backends.backend_agg import FigureCanvasAgg
from matplotlib.patches import Rectangle, Wedge
from matplotlib.collections import PatchCollection
from matplotlib import cm
import subprocess

# Windowsì—ì„œ FFmpeg ì½˜ì†” ì°½ ìˆ¨ê¸°ê¸° ìœ„í•œ ì„¤ì •
SUBPROCESS_STARTUP_INFO = None
SUBPROCESS_CREATION_FLAGS = 0
if os.name == 'nt':
    SUBPROCESS_STARTUP_INFO = subprocess.STARTUPINFO()
    SUBPROCESS_STARTUP_INFO.dwFlags |= subprocess.STARTF_USESHOWWINDOW
    SUBPROCESS_STARTUP_INFO.wShowWindow = subprocess.SW_HIDE
    SUBPROCESS_CREATION_FLAGS = subprocess.CREATE_NO_WINDOW

# FFmpeg ê²½ë¡œë¥¼ PATHì— ì¶”ê°€ (ì—¬ëŸ¬ ê°€ëŠ¥í•œ ê²½ë¡œ ì‹œë„)
import shutil
possible_ffmpeg_paths = [
    r"C:\ProgramData\chocolatey\bin",  # Chocolatey ì„¤ì¹˜ ê²½ë¡œ
    r"C:\ffmpeg\bin",  # ìˆ˜ë™ ì„¤ì¹˜ ì¼ë°˜ ê²½ë¡œ
    r"C:\Program Files\ffmpeg\bin",
]

for path in possible_ffmpeg_paths:
    if os.path.exists(path) and path not in os.environ["PATH"]:
        os.environ["PATH"] = path + os.pathsep + os.environ["PATH"]
        break

from pydub import AudioSegment

from moviepy.video.io.VideoFileClip import VideoFileClip
from moviepy.video.io.ImageSequenceClip import ImageSequenceClip
from moviepy.audio.io.AudioFileClip import AudioFileClip
from moviepy.video.VideoClip import ImageClip, TextClip
from moviepy.video.compositing.CompositeVideoClip import CompositeVideoClip
from moviepy import vfx

# FFmpeg ê²½ë¡œ ëª…ì‹œì  ì„¤ì • (shutil.which()ë¡œ ì‹œìŠ¤í…œì—ì„œ ì°¾ê¸°)
ffmpeg_path = shutil.which("ffmpeg")
ffprobe_path = shutil.which("ffprobe")

if ffmpeg_path:
    AudioSegment.converter = ffmpeg_path
    AudioSegment.ffmpeg = ffmpeg_path
if ffprobe_path:
    AudioSegment.ffprobe = ffprobe_path

from PIL import Image
import requests
from google.cloud import texttospeech
from google.oauth2 import service_account
import uuid
import re
import warnings

# Windowsì—ì„œ ì½˜ì†” ì°½ ìˆ¨ê¸°ê¸° ìœ„í•œ Popen í´ë˜ìŠ¤
# í´ë˜ìŠ¤ ìƒì† ë°©ì‹ìœ¼ë¡œ íŒ¨ì¹˜ (í•¨ìˆ˜ë¡œ êµì²´í•˜ë©´ asyncio.windows_utilsì—ì„œ ìƒì† ì˜¤ë¥˜ ë°œìƒ)
if os.name == 'nt':
    class _PopenNoConsole(subprocess.Popen):
        """ì½˜ì†” ì°½ì„ ìˆ¨ê¸°ëŠ” Popen í´ë˜ìŠ¤"""
        def __init__(self, *args, **kwargs):
            if 'startupinfo' not in kwargs or kwargs['startupinfo'] is None:
                kwargs['startupinfo'] = SUBPROCESS_STARTUP_INFO
            if 'creationflags' not in kwargs or kwargs['creationflags'] == 0:
                kwargs['creationflags'] = SUBPROCESS_CREATION_FLAGS
            super().__init__(*args, **kwargs)

    # pydubê°€ import ì‹œì ì— Popenì„ ë³µì‚¬í•˜ë¯€ë¡œ í•´ë‹¹ ëª¨ë“ˆë„ íŒ¨ì¹˜
    try:
        from pydub import utils as pydub_utils
        if hasattr(pydub_utils, 'Popen'):
            pydub_utils.Popen = _PopenNoConsole
    except Exception as e:
        print(f"[WARNING] pydub íŒ¨ì¹˜ ì‹¤íŒ¨: {e}")

    # MoviePy ffmpeg ëª¨ë“ˆë“¤ íŒ¨ì¹˜ (ì´ë¯¸ importëœ sp.Popen ì°¸ì¡° ì—…ë°ì´íŠ¸)
    try:
        from moviepy.video.io import ffmpeg_writer
        from moviepy.video.io import ffmpeg_reader
        from moviepy.audio.io import ffmpeg_audiowriter

        if hasattr(ffmpeg_writer, 'sp'):
            ffmpeg_writer.sp.Popen = _PopenNoConsole
        if hasattr(ffmpeg_reader, 'sp'):
            ffmpeg_reader.sp.Popen = _PopenNoConsole
        if hasattr(ffmpeg_audiowriter, 'sp'):
            ffmpeg_audiowriter.sp.Popen = _PopenNoConsole
    except Exception as e:
        print(f"[WARNING] MoviePy íŒ¨ì¹˜ ì‹¤íŒ¨: {e}")

import studio_utils as utils
import studio_config as config
# from ui_dialogs import CompletionDialog
from studio_config import TEMP_DIR

# numpy ê²½ê³  ë©”ì‹œì§€ ì–µì œ (ë¹„ì£¼ì–¼ë¼ì´ì € ë Œë”ë§ ì‹œ ë¬´ìŒ êµ¬ê°„ ì²˜ë¦¬)
warnings.filterwarnings('ignore', category=RuntimeWarning, message='Mean of empty slice')
warnings.filterwarnings('ignore', category=RuntimeWarning, message='invalid value encountered in divide')

class MoviePyLogger:
    def __init__(self, app, is_batch=False):
        self.app = app
        self.is_batch = is_batch

    def __call__(self, *args, **kwargs):
        message = kwargs.get('message')
        if message and "audio" not in message and "Done." not in message:
            self.app.log_message(message)
        pass

    def write(self, s):
        s = s.strip()
        if not s or 't:' not in s or '%' not in s: return
        try:
            match = re.search(r'(\d+)\%', s)
            if match:
                percent = int(match.group(1))
                overall_progress = 85 + (percent / 100.0) * 15
                self.app.update_progress(f"ìµœì¢… ì˜ìƒ ì¸ì½”ë”© ì¤‘... {percent}%", overall_progress, self.is_batch)
        except Exception: pass

    def flush(self): pass
    
    def iter_bar(self, t=None, chunk=None, **kwargs):
        """moviepy í˜¸í™˜ iter_bar"""
        if t is not None:
            return t
        if chunk is not None:
            return chunk
        return range(0)

# services.pyì˜ 72-149ë²ˆ ì¤„ì„ ì•„ë˜ ì½”ë“œë¡œ êµì²´í•˜ì„¸ìš”
# ì•ˆì •ì„± ìµœìš°ì„  ê°œì„  ë²„ì „

import time
import random
import threading
import asyncio

# TTS API í‚¤ ì‚¬ìš©ëŸ‰ ì¶”ì  ëª¨ë“ˆ
try:
    import tts_quota_manager as quota_manager
    TTS_QUOTA_ENABLED = True
    print("[studio_services] TTS Quota Manager ì—°ë™ ì™„ë£Œ")
except ImportError:
    TTS_QUOTA_ENABLED = False
    print("[studio_services] TTS Quota Manager ë¯¸ì‚¬ìš© (ê¸°ì¡´ ë°©ì‹)")

# Google TTS API ì œí•œ: 5000 bytes
# ì•ˆì •ì„±ì„ ìœ„í•´ ë” ë³´ìˆ˜ì ìœ¼ë¡œ ì„¤ì • (4800 â†’ 4000)
TTS_SAFE_LIMIT_BYTES = 4000

def _monitor_encoding_progress(output_path, app, stop_event, duration_seconds, total_frames=None, fps=30):
    """
    ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì¸ì½”ë”© ì§„í–‰ë¥ ì„ ëª¨ë‹ˆí„°ë§
    ì˜ìƒ ê¸¸ì´ ê¸°ë°˜ ì˜ˆìƒ ì‹œê°„ìœ¼ë¡œ ì„ í˜• ì§„í–‰ë¥  í‘œì‹œ (85% ~ 99%)
    """
    start_time = time.time()
    last_update_time = start_time

    # ì¸ì½”ë”© ì˜ˆìƒ ì‹œê°„ ê³„ì‚° (ì˜ìƒ ê¸¸ì´ì˜ ì•½ 0.5~1ë°° ì‹œê°„ ì†Œìš”ë¡œ ê°€ì •)
    # ì‹¤ì œë¡œëŠ” í•˜ë“œì›¨ì–´ì— ë”°ë¼ ë‹¤ë¥´ì§€ë§Œ í‰ê· ì ìœ¼ë¡œ ì˜ìƒ ê¸¸ì´ì™€ ë¹„ìŠ·í•˜ê±°ë‚˜ ì§§ìŒ
    estimated_encoding_time = max(duration_seconds * 0.8, 10)  # ìµœì†Œ 10ì´ˆ

    # Eelì„ í†µí•´ í”„ë¡ íŠ¸ì—”ë“œì— ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
    try:
        import eel
        has_eel = True
    except:
        has_eel = False

    # ì´ í”„ë ˆì„ ìˆ˜ ê³„ì‚° (ì—†ìœ¼ë©´ durationìœ¼ë¡œ ì¶”ì •)
    if total_frames is None:
        total_frames = int(duration_seconds * fps)

    while not stop_event.is_set():
        time.sleep(1)  # 1ì´ˆë§ˆë‹¤ ì²´í¬

        try:
            now = time.time()
            elapsed = now - start_time
            elapsed_int = int(elapsed)
            elapsed_min = elapsed_int // 60
            elapsed_sec = elapsed_int % 60

            # ì‹œê°„ ê¸°ë°˜ ì„ í˜• ì§„í–‰ë¥  (ìµœëŒ€ 99%ê¹Œì§€)
            # ì¸ì½”ë”©ì€ ì˜ˆìƒ ì‹œê°„ ë‚´ì— ì™„ë£Œë˜ëŠ” ê²ƒìœ¼ë¡œ ê°€ì •í•˜ê³  ì„ í˜• ì¦ê°€
            progress = min(99, (elapsed / estimated_encoding_time) * 100)

            # ì§„í–‰ë°” ì—…ë°ì´íŠ¸ (85% ~ 99%)
            bar_progress = 85 + (progress * 0.14)

            # íŒŒì¼ í¬ê¸° í‘œì‹œ (ìˆìœ¼ë©´)
            size_text = ""
            if os.path.exists(output_path):
                try:
                    current_size = os.path.getsize(output_path)
                    size_mb = current_size / (1024 * 1024)
                    size_text = f" ({size_mb:.1f}MB)"
                except:
                    pass

            detail_text = f"ê²½ê³¼ ì‹œê°„: {elapsed_min}ë¶„ {elapsed_sec}ì´ˆ{size_text}"

            # Eelì„ í†µí•´ í”„ë¡ íŠ¸ì—”ë“œ ì—…ë°ì´íŠ¸
            if has_eel:
                try:
                    eel.studioUpdateProgressFromPython(bar_progress, f"ì¸ì½”ë”© ì¤‘... {int(progress)}%", detail_text)
                except:
                    pass  # Eel í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ

            last_update_time = now
        except Exception:
            pass  # ì˜¤ë¥˜ ë¬´ì‹œ

def validate_api_key(secret):
    try:
        _synthesize_chunk(secret, "test", "en-US-Standard-A", 1.0, 0.0, is_ssml=False)
        return True, "API í‚¤ê°€ ìœ íš¨í•©ë‹ˆë‹¤."
    except Exception as e:
        return False, f"API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ë„¤íŠ¸ì›Œí¬ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.\n\nì˜¤ë¥˜: {e}"

def _synthesize_chunk(secret, text, api_voice, rate, pitch, volume_gain_db=0, is_ssml=False, max_retries=5):
    """
    ì•ˆì •ì ì¸ TTS API í˜¸ì¶œ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)

    Args:
        rate: ì†ë„ (0.25 ~ 4.0)
        pitch: í”¼ì¹˜ (-20 ~ 20)
        volume_gain_db: ë³¼ë¥¨ ê²Œì¸ dB (-10 ~ 10, ê¸°ë³¸ê°’: 0)
        max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸ê°’: 5)
    """
    # Chirp3-HD, Chirp-HD, Studio ëª¨ë¸ì€ ì†ë„/í”¼ì¹˜ ì¡°ì ˆ ë¶ˆê°€
    is_unsupported_voice = "Chirp" in api_voice or "Studio" in api_voice
    language_code = "-".join(api_voice.split('-', 2)[:2])

    # ì„œë¹„ìŠ¤ ê³„ì • JSON íŒŒì¼ ì‚¬ìš© (ë” ì•ˆì •ì )
    if os.path.isabs(secret) and os.path.exists(secret) and secret.lower().endswith(".json"):
        creds = service_account.Credentials.from_service_account_file(secret)
        client = texttospeech.TextToSpeechClient(credentials=creds)
        audio_config_args = {'audio_encoding': texttospeech.AudioEncoding.MP3}
        if not is_unsupported_voice and not is_ssml:
            audio_config_args['speaking_rate'] = rate
            audio_config_args['pitch'] = pitch
        # ë³¼ë¥¨ ê²Œì¸ì€ í•­ìƒ ì ìš© ê°€ëŠ¥ (-96.0 ~ 16.0 dB)
        if volume_gain_db != 0:
            audio_config_args['volume_gain_db'] = float(volume_gain_db)
        audio_config = texttospeech.AudioConfig(**audio_config_args)
        synthesis_input = texttospeech.SynthesisInput(ssml=text) if is_ssml else texttospeech.SynthesisInput(text=text)
        
        # SDK í˜¸ì¶œë„ ì¬ì‹œë„ ë¡œì§ ì ìš©
        for attempt in range(max_retries):
            try:
                resp = client.synthesize_speech(
                    input=synthesis_input, 
                    voice=texttospeech.VoiceSelectionParams(language_code=language_code, name=api_voice), 
                    audio_config=audio_config
                )
                return resp.audio_content
            except Exception as e:
                if attempt < max_retries - 1:
                    wait_time = (2 ** attempt) + random.uniform(0, 1)  # Exponential backoff
                    time.sleep(wait_time)
                else:
                    raise
    
    # REST API ì‚¬ìš© (ì¬ì‹œë„ ë¡œì§ ê°•í™”)
    url = f"https://texttospeech.googleapis.com/v1/text:synthesize?key={secret}"
    audio_config_payload = {"audioEncoding": "MP3"}
    if not is_unsupported_voice and not is_ssml:
        audio_config_payload["speakingRate"] = rate
        audio_config_payload["pitch"] = pitch
    # ë³¼ë¥¨ ê²Œì¸ì€ í•­ìƒ ì ìš© ê°€ëŠ¥
    if volume_gain_db != 0:
        audio_config_payload["volumeGainDb"] = float(volume_gain_db)
    input_payload = {"ssml": text} if is_ssml else {"text": text}
    payload = {
        "input": input_payload,
        "voice": {"languageCode": language_code, "name": api_voice},
        "audioConfig": audio_config_payload
    }
    
    for attempt in range(max_retries):
        try:
            r = requests.post(url, json=payload, timeout=90)  # íƒ€ì„ì•„ì›ƒ 60â†’90ì´ˆë¡œ ì¦ê°€
            r.raise_for_status()
            data = r.json()
            
            if "audioContent" not in data:
                raise RuntimeError(f"TTS REST ì‘ë‹µì— audioContentê°€ ì—†ìŠµë‹ˆë‹¤: {data}")
            
            return base64.b64decode(data["audioContent"])
            
        except requests.exceptions.HTTPError as e:
            status_code = e.response.status_code if e.response else 0
            
            # ì¬ì‹œë„ ê°€ëŠ¥í•œ ì—ëŸ¬ì¸ì§€ íŒë‹¨
            if status_code in [500, 502, 503, 504]:  # ì„œë²„ ì—ëŸ¬
                if attempt < max_retries - 1:
                    wait_time = (2 ** attempt) * 2 + random.uniform(0, 2)  # ë” ê¸´ ëŒ€ê¸°
                    print(f"[ì¬ì‹œë„ {attempt+1}/{max_retries}] ì„œë²„ ì—ëŸ¬ ({status_code}), {wait_time:.1f}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(wait_time)
                else:
                    raise
            elif status_code == 429:  # Rate limit
                if attempt < max_retries - 1:
                    wait_time = (2 ** attempt) * 3 + random.uniform(0, 3)  # í›¨ì”¬ ë” ê¸´ ëŒ€ê¸°
                    print(f"[ì¬ì‹œë„ {attempt+1}/{max_retries}] Rate limit ì´ˆê³¼, {wait_time:.1f}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(wait_time)
                else:
                    raise
            elif status_code == 400:  # Bad Request - ìƒì„¸ ë¡œê¹…
                error_details = ""
                try:
                    error_data = e.response.json()
                    error_details = f"\nìƒì„¸ ì˜¤ë¥˜: {error_data}"
                except:
                    error_details = f"\nì‘ë‹µ ë‚´ìš©: {e.response.text[:500]}"

                print(f"[400 Bad Request] í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)}ì")
                print(f"[400 Bad Request] í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {text[:100]}...")
                print(f"[400 Bad Request]{error_details}")
                raise RuntimeError(f"400 Bad Request: í…ìŠ¤íŠ¸ê°€ TTS APIì—ì„œ ê±°ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤.{error_details}")
            else:
                # ì¬ì‹œë„ ë¶ˆê°€ëŠ¥í•œ ì—ëŸ¬ (401, 403 ë“±)
                raise
                
        except (requests.exceptions.Timeout, requests.exceptions.ConnectionError) as e:
            if attempt < max_retries - 1:
                wait_time = (2 ** attempt) + random.uniform(0, 1)
                print(f"[ì¬ì‹œë„ {attempt+1}/{max_retries}] ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬, {wait_time:.1f}ì´ˆ í›„ ì¬ì‹œë„...")
                time.sleep(wait_time)
            else:
                raise

# --- Edge TTS í•©ì„± í•¨ìˆ˜ (ë¬´ë£Œ) ---
def synthesize_edge_tts_bytes(text, voice_name, rate=1.0, pitch=0.0, app=None, pause_after_ms=0):
    """
    Edge TTSë¥¼ ì‚¬ìš©í•œ ìŒì„± í•©ì„± (ë¬´ë£Œ)

    Args:
        text: í•©ì„±í•  í…ìŠ¤íŠ¸
        voice_name: Edge TTS ìŒì„± ì´ë¦„ (ì˜ˆ: "ko-KR-SunHiNeural")
        rate: ì†ë„ (0.5 ~ 2.0, ê¸°ë³¸ê°’ 1.0)
        pitch: í”¼ì¹˜ ì¡°ì ˆ (-50 ~ +50 Hz, ê¸°ë³¸ê°’ 0)
        app: ë¡œê·¸ ì¶œë ¥ìš© ì•± ê°ì²´
        pause_after_ms: ë¬¸ì¥ í›„ ì‰¬ëŠ” ì‹œê°„ (ë°€ë¦¬ì´ˆ, ê¸°ë³¸ê°’: 0)

    Returns:
        bytes: MP3 ì˜¤ë””ì˜¤ ë°ì´í„°
    """
    try:
        import edge_tts
    except ImportError:
        raise ImportError("edge-tts íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install edge-tts'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")

    # í…ìŠ¤íŠ¸ ìœ íš¨ì„± ê²€ì‚¬
    if not text or not text.strip():
        raise ValueError("í•©ì„±í•  í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

    # ìŒì„± ì´ë¦„ ìœ íš¨ì„± ê²€ì‚¬
    if not voice_name or not voice_name.strip():
        raise ValueError("Edge TTS ìŒì„±ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    async def _synthesize():
        # rateë¥¼ í¼ì„¼íŠ¸ ë¬¸ìì—´ë¡œ ë³€í™˜ (1.0 = +0%, 1.5 = +50%, 0.5 = -50%)
        rate_percent = int((rate - 1.0) * 100)
        rate_str = f"+{rate_percent}%" if rate_percent >= 0 else f"{rate_percent}%"

        # pitchë¥¼ Hz ë¬¸ìì—´ë¡œ ë³€í™˜
        pitch_hz = int(pitch)
        pitch_str = f"+{pitch_hz}Hz" if pitch_hz >= 0 else f"{pitch_hz}Hz"

        communicate = edge_tts.Communicate(text.strip(), voice_name.strip(), rate=rate_str, pitch=pitch_str)

        audio_data = b""
        async for chunk in communicate.stream():
            if chunk["type"] == "audio":
                audio_data += chunk["data"]

        if not audio_data:
            raise RuntimeError(f"Edge TTSì—ì„œ ì˜¤ë””ì˜¤ë¥¼ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìŒì„±: {voice_name}")

        return audio_data

    if app:
        app.log_message(f"  â†’ Edge TTS í•©ì„± ì¤‘... (ìŒì„±: {voice_name}, í…ìŠ¤íŠ¸: {len(text)}ì)")

    # asyncio ì´ë²¤íŠ¸ ë£¨í”„ ì²˜ë¦¬ (ìƒˆ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)
    import concurrent.futures
    with concurrent.futures.ThreadPoolExecutor() as executor:
        future = executor.submit(asyncio.run, _synthesize())
        audio_bytes = future.result()

    # pause_after ì ìš©
    if pause_after_ms > 0:
        audio_seg = AudioSegment.from_mp3(io.BytesIO(audio_bytes))
        audio_seg += AudioSegment.silent(duration=pause_after_ms)
        byte_io = io.BytesIO()
        audio_seg.export(byte_io, format="mp3")
        return byte_io.getvalue()

    return audio_bytes

def is_edge_tts_voice(voice_name):
    """Edge TTS ìŒì„±ì¸ì§€ í™•ì¸ (ì˜ˆ: ko-KR-SunHiNeural, en-US-JennyNeural)"""
    if not voice_name:
        return False
    # Edge TTS ìŒì„±ì€ "Neural"ë¡œ ëë‚˜ê³ , Google TTS íŒ¨í„´ì´ ì•„ë‹˜
    google_patterns = ["Chirp", "Wavenet", "Standard", "Studio", "News", "Casual", "Polyglot", "Neural2"]
    if any(p in voice_name for p in google_patterns):
        return False
    return voice_name.endswith("Neural")

def synthesize_tts_bytes(profile_name, text, api_voice, rate, pitch, volume_gain_db=0, is_ssml=False, app=None, use_edge_tts=False, pause_after_ms=0):
    """
    ì•ˆì •ì ì¸ TTS ìŒì„± í•©ì„± (ê¸´ í…ìŠ¤íŠ¸ ìë™ ë¶„í•  + ì¬ì‹œë„)

    Args:
        rate: ì†ë„ (0.25 ~ 4.0)
        pitch: í”¼ì¹˜ (-20 ~ 20)
        volume_gain_db: ë³¼ë¥¨ ê²Œì¸ dB (-10 ~ 10, ê¸°ë³¸ê°’: 0)
        use_edge_tts: Trueì´ë©´ Edge TTS ì‚¬ìš© (ë¬´ë£Œ, API í‚¤ ë¶ˆí•„ìš”)
        pause_after_ms: ë¬¸ì¥ í›„ ì‰¬ëŠ” ì‹œê°„ (ë°€ë¦¬ì´ˆ, ê¸°ë³¸ê°’: 0)
    """
    # Edge TTS ì‚¬ìš© ì‹œ (ë¬´ë£Œ) - Edge TTSëŠ” volume_gain_db ë¯¸ì§€ì›
    if use_edge_tts or is_edge_tts_voice(api_voice):
        return synthesize_edge_tts_bytes(text, api_voice, rate, pitch, app, pause_after_ms)

    # Google TTS ì‚¬ìš© ì‹œ - Quota Managerë¥¼ í†µí•œ ìë™ í‚¤ ì„ íƒ
    secret = None
    key_id = None
    char_count = len(text)

    if TTS_QUOTA_ENABLED:
        # ì‚¬ìš© ê°€ëŠ¥í•œ API í‚¤ ìë™ ì„ íƒ
        available_key = quota_manager.get_available_api_key(api_voice, char_count)
        if available_key:
            secret = available_key['api_key']
            key_id = available_key['key_id']
            if app and available_key.get('warning'):
                app.log_message(f"  âš ï¸ {available_key['warning']}")
            if app:
                app.log_message(f"  ğŸ”‘ TTS í‚¤ ì‚¬ìš©: {available_key['name']}")
        else:
            if app:
                app.log_message("  âš ï¸ ìë™ í‚¤ ì„ íƒ ë¶ˆê°€ - í”„ë¡œí•„ í‚¤ ì‚¬ìš©")

    # Quota Managerì—ì„œ í‚¤ë¥¼ ëª» ì°¾ì•˜ìœ¼ë©´ ê¸°ì¡´ í”„ë¡œí•„ ì‚¬ìš©
    if not secret:
        secret = utils.get_profiles().get(profile_name, "").strip()
        if not secret:
            raise ValueError(f"'{profile_name}' í”„ë¡œí•„ ê°’ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

    text_length = len(text)
    text_bytes = len(text.encode('utf-8'))

    # ì‚¬ìš©ëŸ‰ ì¶”ì  í—¬í¼ í•¨ìˆ˜
    def _track_usage(char_count):
        if TTS_QUOTA_ENABLED and key_id:
            quota_manager.add_usage(key_id, api_voice, char_count)

    # SSML ì²˜ë¦¬
    if is_ssml:
        if app:
            app.log_message(f"  â†’ SSML ìŒì„± í•©ì„± ì¤‘... (í…ìŠ¤íŠ¸ ê¸¸ì´: {text_length}ì)")
        audio_bytes = _synthesize_chunk(secret, text, api_voice, 1.0, 0.0, volume_gain_db, is_ssml=True)
        _track_usage(text_length)  # ì‚¬ìš©ëŸ‰ ê¸°ë¡
        # pause_after ì ìš©
        if pause_after_ms > 0:
            audio_seg = AudioSegment.from_mp3(io.BytesIO(audio_bytes))
            audio_seg += AudioSegment.silent(duration=pause_after_ms)
            byte_io = io.BytesIO()
            audio_seg.export(byte_io, format="mp3")
            return byte_io.getvalue()
        return audio_bytes

    # ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ë°”ë¡œ ì²˜ë¦¬
    if text_bytes <= TTS_SAFE_LIMIT_BYTES:
        if app:
            app.log_message(f"  â†’ TTS API í˜¸ì¶œ ì¤‘... (í…ìŠ¤íŠ¸: {text_length}ì, {text_bytes} bytes)")
        audio_bytes = _synthesize_chunk(secret, text, api_voice, rate, pitch, volume_gain_db, is_ssml=False)
        _track_usage(text_length)  # ì‚¬ìš©ëŸ‰ ê¸°ë¡
        # pause_after ì ìš©
        if pause_after_ms > 0:
            audio_seg = AudioSegment.from_mp3(io.BytesIO(audio_bytes))
            audio_seg += AudioSegment.silent(duration=pause_after_ms)
            byte_io = io.BytesIO()
            audio_seg.export(byte_io, format="mp3")
            return byte_io.getvalue()
        return audio_bytes
    
    # ê¸´ í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í•  (ë‹¤ì¤‘ êµ¬ë‘ì  ì§€ì›)
    import re

    # ì—¬ëŸ¬ êµ¬ë‘ì ìœ¼ë¡œ ë¬¸ì¥ ë¶„í•  (í•œêµ­ì–´/ì˜ì–´ ì§€ì›)
    # ë§ˆì¹¨í‘œ, ë¬¼ìŒí‘œ, ëŠë‚Œí‘œ, ì‰¼í‘œ, ì„¸ë¯¸ì½œë¡  ë“±
    sentence_pattern = r'([^.!?\n]+[.!?\n]+|[^,;]+[,;]+)'
    raw_sentences = re.findall(sentence_pattern, text)

    # ë¶„í• ë˜ì§€ ì•Šì€ ë‚˜ë¨¸ì§€ í…ìŠ¤íŠ¸ ì²˜ë¦¬
    if raw_sentences:
        remaining = text
        for s in raw_sentences:
            remaining = remaining.replace(s, '', 1)
        if remaining.strip():
            raw_sentences.append(remaining.strip())
    else:
        # íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ìœ¼ë¡œ ì·¨ê¸‰
        raw_sentences = [text]

    chunks, current_chunk = [], ""

    for sentence in raw_sentences:
        if not sentence.strip():
            continue

        # í˜„ì¬ ì²­í¬ì— ë¬¸ì¥ ì¶”ê°€ ì‹œë„
        test_chunk = current_chunk + sentence
        test_bytes = len(test_chunk.encode('utf-8'))

        if test_bytes > TTS_SAFE_LIMIT_BYTES:
            # í˜„ì¬ ì²­í¬ ì €ì¥
            if current_chunk:
                chunks.append(current_chunk)
                current_chunk = sentence
            else:
                # ë‹¨ì¼ ë¬¸ì¥ì´ ë„ˆë¬´ í¼ -> ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„í• 
                words = sentence.split()
                word_chunk = ""
                for word in words:
                    test_word_chunk = word_chunk + word + " "
                    if len(test_word_chunk.encode('utf-8')) > TTS_SAFE_LIMIT_BYTES:
                        if word_chunk:
                            chunks.append(word_chunk.strip())
                        word_chunk = word + " "
                    else:
                        word_chunk = test_word_chunk
                if word_chunk:
                    current_chunk = word_chunk.strip()
        else:
            current_chunk = test_chunk

    if current_chunk:
        chunks.append(current_chunk)
    
    if app:
        app.log_message(f"  â†’ ê¸´ í…ìŠ¤íŠ¸ ê°ì§€: {text_length}ì ({text_bytes} bytes)")
        app.log_message(f"  â†’ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
        app.log_message(f"  â†’ API ì•ˆì •ì„±ì„ ìœ„í•´ ê° ì²­í¬ ê°„ 0.5ì´ˆ ëŒ€ê¸°í•©ë‹ˆë‹¤.")
    
    # ì²­í¬ ê²€ì¦ ë° ì •ë¦¬
    validated_chunks = []
    for chunk in chunks:
        chunk = chunk.strip()
        if not chunk:  # ë¹ˆ ì²­í¬ ì œê±°
            continue
        chunk_bytes = len(chunk.encode('utf-8'))

        # ë„ˆë¬´ ì‘ì€ ì²­í¬ëŠ” ê²½ê³  (10ë°”ì´íŠ¸ ë¯¸ë§Œ)
        if chunk_bytes < 10:
            if app:
                app.log_message(f"  âš ï¸ ê²½ê³ : ë„ˆë¬´ ì‘ì€ ì²­í¬ ë°œê²¬ ({chunk_bytes} bytes), ê±´ë„ˆëœ€: {chunk[:50]}")
            continue

        # ë„ˆë¬´ í° ì²­í¬ëŠ” ì—ëŸ¬
        if chunk_bytes > TTS_SAFE_LIMIT_BYTES:
            if app:
                app.log_message(f"  âŒ ì˜¤ë¥˜: ì²­í¬ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤ ({chunk_bytes} bytes). ë¬¸ì¥ì„ ë” ì§§ê²Œ ë‚˜ëˆ ì£¼ì„¸ìš”.")
            raise ValueError(f"ì²­í¬ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤: {chunk_bytes} bytes (ìµœëŒ€ {TTS_SAFE_LIMIT_BYTES} bytes)")

        validated_chunks.append(chunk)

    if not validated_chunks:
        raise ValueError("ìœ íš¨í•œ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

    if app:
        app.log_message(f"  âœ“ {len(validated_chunks)}ê°œì˜ ìœ íš¨í•œ ì²­í¬ ì¤€ë¹„ ì™„ë£Œ")

    # ì²­í¬ë³„ ì²˜ë¦¬ (ì•ˆì •ì„± ìµœìš°ì„ )
    combined_audio = AudioSegment.empty()

    for idx, chunk in enumerate(validated_chunks):
        # ì¤‘ì§€ ìš”ì²­ í™•ì¸
        if app and hasattr(app, 'cancel_event') and app.cancel_event.is_set():
            if app:
                app.log_message(f"  âš ï¸ TTS ì²˜ë¦¬ ì¤‘ ì¤‘ì§€ë¨ (ì²­í¬ {idx+1}/{len(validated_chunks)})")
            raise RuntimeError("ì‚¬ìš©ìì— ì˜í•´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤")

        if not chunk.strip():
            continue

        if app:
            app.log_message(f"     â€¢ ì²­í¬ {idx+1}/{len(validated_chunks)} ì²˜ë¦¬ ì¤‘...")

        # API í˜¸ì¶œ (ì¬ì‹œë„ ë¡œì§ ë‚´ì¥)
        try:
            audio_bytes = _synthesize_chunk(secret, chunk, api_voice, rate, pitch, volume_gain_db, is_ssml=False)
            combined_audio += AudioSegment.from_mp3(io.BytesIO(audio_bytes))

            if app:
                app.log_message(f"       âœ“ ì„±ê³µ (í¬ê¸°: {len(audio_bytes)} bytes)")

            # Rate Limit ë°©ì§€: ì²­í¬ ê°„ ì§§ì€ ëŒ€ê¸° (0.5ì´ˆ)
            if idx < len(validated_chunks) - 1:  # ë§ˆì§€ë§‰ ì²­í¬ê°€ ì•„ë‹ˆë©´
                # ëŒ€ê¸° ì¤‘ì—ë„ ì¤‘ì§€ í™•ì¸ (0.1ì´ˆ ë‹¨ìœ„ë¡œ 5ë²ˆ = 0.5ì´ˆ)
                for _ in range(5):
                    if app and hasattr(app, 'cancel_event') and app.cancel_event.is_set():
                        if app:
                            app.log_message(f"  âš ï¸ ëŒ€ê¸° ì¤‘ ì¤‘ì§€ë¨")
                        raise RuntimeError("ì‚¬ìš©ìì— ì˜í•´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤")
                    time.sleep(0.1)

        except Exception as e:
            if app:
                app.log_message(f"       âœ— ì²­í¬ {idx+1} ì²˜ë¦¬ ìµœì¢… ì‹¤íŒ¨: {e}")
            raise  # ì‹¤íŒ¨ ì‹œ ì „ì²´ ì‘ì—… ì¤‘ë‹¨

    if app:
        app.log_message(f"  â†’ ëª¨ë“  ì²­í¬ ê²°í•© ì™„ë£Œ! (ì´ {len(validated_chunks)}ê°œ)")

    # ì „ì²´ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì‚¬ìš©ëŸ‰ ê¸°ë¡
    _track_usage(text_length)

    # pause_after ì ìš©
    if pause_after_ms > 0:
        combined_audio += AudioSegment.silent(duration=pause_after_ms)

    byte_io = io.BytesIO()
    combined_audio.export(byte_io, format="mp3")
    return byte_io.getvalue()

def generate_single_clip_audio(app_tab, cid, api_key_profile_name=None):
    clip = app_tab._get_clip_by_id(cid)
    if not clip: return None, "í´ë¦½ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    char, text = clip["character"], clip["text"]; is_ssml = clip.get("is_ssml", False)
    if char not in app_tab.character_widgets: return None, f"'{char}' ìºë¦­í„° ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤."

    w = app_tab.character_widgets[char]
    if w["voice_var"].get() == "ìŒì„± ì„ íƒ": return None, f"'{char}' ìºë¦­í„°ì˜ ìŒì„±ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    try:
        profile_name = api_key_profile_name or app_tab.profile_var.get()
        if not profile_name: raise ValueError("TTS ì‘ì—…ìš© í”„ë¡œí•„ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")

        selected_lang, selected_group, selected_voice_ui = w['lang_var'].get(), w['group_var'].get(), w['voice_var'].get()
        api_voice = next((name for name, gender in config.LANG_VOICE_GROUPS.get(selected_lang, {}).get(selected_group, {}).items() if app_tab._format_voice_name_internal(name, gender) == selected_voice_ui), None)
        if not api_voice: raise ValueError(f"ì„ íƒëœ ìŒì„± '{selected_voice_ui}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        bytes_ = synthesize_tts_bytes(profile_name, text, api_voice, w["speed_var"].get(), w["pitch_var"].get(), volume_gain_db=0, is_ssml=is_ssml, app=getattr(app_tab, 'app', None))
        return AudioSegment.from_mp3(io.BytesIO(bytes_)), None
    except Exception as e:
        return None, f"ì˜¤ë””ì˜¤ ìƒì„± ì¤‘ API ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

def generate_srt_from_audio(audio_path, output_srt_path, app=None, model_size="base"):
    """
    Whisperë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ íŒŒì¼ì—ì„œ SRT ìë§‰ ìƒì„±

    Args:
        audio_path: ì…ë ¥ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ (MP3 ë“±)
        output_srt_path: ì¶œë ¥ SRT íŒŒì¼ ê²½ë¡œ
        app: ë¡œê·¸ ì¶œë ¥ì„ ìœ„í•œ ì•± ê°ì²´ (ì˜µì…˜)
        model_size: Whisper ëª¨ë¸ í¬ê¸° ("tiny", "base", "small", "medium", "large")
                    ê¸°ë³¸ê°’ì€ "base" (ì†ë„ì™€ ì •í™•ë„ì˜ ê· í˜•)

    Returns:
        True if successful, False otherwise
    """
    try:
        import whisper

        if app:
            app.log_message(f"\nğŸ“ Whisper STT ìë§‰ ìƒì„± ì‹œì‘...")
            app.log_message(f"  ëª¨ë¸: {model_size}")
            app.log_message(f"  ì…ë ¥: {audio_path}")

        # Whisper ëª¨ë¸ ë¡œë“œ
        if app:
            app.log_message(f"  â†’ Whisper ëª¨ë¸ ë¡œë”© ì¤‘... (ìµœì´ˆ ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ)")

        model = whisper.load_model(model_size)

        if app:
            app.log_message(f"  âœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            app.log_message(f"  â†’ ìŒì„± ì¸ì‹ ì‹œì‘...")

        # ìŒì„± ì¸ì‹ ì‹¤í–‰
        result = model.transcribe(
            audio_path,
            language="ko",  # í•œêµ­ì–´ ì§€ì • (ì •í™•ë„ í–¥ìƒ)
            task="transcribe",  # ë²ˆì—­ì´ ì•„ë‹Œ ì „ì‚¬
            verbose=False  # ìƒì„¸ ë¡œê·¸ ë¹„í™œì„±í™”
        )

        if app:
            app.log_message(f"  âœ“ ìŒì„± ì¸ì‹ ì™„ë£Œ")
            app.log_message(f"  â†’ SRT íŒŒì¼ ìƒì„± ì¤‘...")

        # SRT í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        def format_timestamp(seconds):
            """ì´ˆ ë‹¨ìœ„ ì‹œê°„ì„ SRT íƒ€ì„ìŠ¤íƒ¬í”„ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (HH:MM:SS,mmm)"""
            hours = int(seconds // 3600)
            minutes = int((seconds % 3600) // 60)
            secs = int(seconds % 60)
            millis = int((seconds - int(seconds)) * 1000)
            return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"

        # SRT íŒŒì¼ ì‘ì„±
        with open(output_srt_path, 'w', encoding='utf-8') as srt_file:
            for i, segment in enumerate(result['segments'], start=1):
                start_time = format_timestamp(segment['start'])
                end_time = format_timestamp(segment['end'])
                text = segment['text'].strip()

                # SRT í˜•ì‹: ë²ˆí˜¸, íƒ€ì„ìŠ¤íƒ¬í”„, í…ìŠ¤íŠ¸, ë¹ˆ ì¤„
                srt_file.write(f"{i}\n")
                srt_file.write(f"{start_time} --> {end_time}\n")
                srt_file.write(f"{text}\n\n")

        if app:
            app.log_message(f"  âœ“ SRT íŒŒì¼ ìƒì„± ì™„ë£Œ: {output_srt_path}")
            app.log_message(f"  ğŸ“Š ì´ {len(result['segments'])}ê°œì˜ ìë§‰ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±ë¨")

        return True

    except ImportError:
        if app:
            app.log_message(f"  âŒ ì˜¤ë¥˜: openai-whisper íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            app.log_message(f"     ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”: pip install openai-whisper")
        return False
    except Exception as e:
        import traceback
        if app:
            app.log_message(f"  âŒ SRT ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            app.log_message(f"{traceback.format_exc()}")
        return False

def generate_srt_from_clips(clips, audio_segments, output_srt_path, app=None, max_chars=35):
    """
    í´ë¦½ ë°ì´í„°ì™€ ì˜¤ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì›ë³¸ í…ìŠ¤íŠ¸ ê¸°ë°˜ ì •í™•í•œ SRT ìë§‰ ìƒì„±

    Args:
        clips: í´ë¦½ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ [{'character': 'ìºë¦­í„°ëª…', 'text': 'ëŒ€ì‚¬'}]
        audio_segments: AudioSegment ë¦¬ìŠ¤íŠ¸ (ê° í´ë¦½ë³„ ì˜¤ë””ì˜¤)
        output_srt_path: ì¶œë ¥ SRT íŒŒì¼ ê²½ë¡œ
        app: ë¡œê·¸ ì¶œë ¥ì„ ìœ„í•œ ì•± ê°ì²´ (ì˜µì…˜)
        max_chars: ìë§‰ í•œ ì¤„ ìµœëŒ€ ê¸€ì ìˆ˜ (ê¸°ë³¸ê°’: 35, ì•½ 1-2ì¤„)

    Returns:
        True if successful, False otherwise
    """
    try:
        if app:
            app.log_message(f"\nğŸ“ SRT ìë§‰ ìƒì„± ì‹œì‘...")

        def format_timestamp(seconds):
            """ì´ˆ ë‹¨ìœ„ ì‹œê°„ì„ SRT íƒ€ì„ìŠ¤íƒ¬í”„ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (HH:MM:SS,mmm)"""
            hours = int(seconds // 3600)
            minutes = int((seconds % 3600) // 60)
            secs = int(seconds % 60)
            millis = int((seconds - int(seconds)) * 1000)
            return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"

        def split_text_smartly(text, max_length):
            """í…ìŠ¤íŠ¸ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ë¶„í•  (ë‹¨ì–´ ì¤‘ê°„ ì ˆëŒ€ ì•ˆ ìë¦„)"""
            # ì´ë¯¸ ì§§ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
            if len(text) <= max_length:
                return [text]

            segments = []

            # 1ë‹¨ê³„: ë¬¸ì¥ ë¶€í˜¸ë¡œ ë¨¼ì € ë¶„í•  (., !, ?)
            sentence_endings = ['. ', '! ', '? ', '.\n', '!\n', '?\n']
            sentences = []
            current = ""

            i = 0
            while i < len(text):
                current += text[i]
                # ë¬¸ì¥ ë¶€í˜¸ ì²´í¬
                found_ending = False
                for ending in sentence_endings:
                    if current.endswith(ending):
                        sentences.append(current.strip())
                        current = ""
                        found_ending = True
                        break
                i += 1

            # ë‚¨ì€ í…ìŠ¤íŠ¸ ì¶”ê°€
            if current.strip():
                sentences.append(current.strip())

            # ë¬¸ì¥ì´ ì—†ìœ¼ë©´ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ë¬¸ì¥ìœ¼ë¡œ
            if not sentences:
                sentences = [text]

            # 2ë‹¨ê³„: ê° ë¬¸ì¥ì´ max_lengthë¥¼ ë„˜ìœ¼ë©´ ì¶”ê°€ ë¶„í• 
            for sentence in sentences:
                if len(sentence) <= max_length:
                    segments.append(sentence)
                else:
                    # ì‰¼í‘œë¡œ ë‚˜ëˆ„ê¸° ì‹œë„
                    if ',' in sentence:
                        parts = sentence.split(',')
                        temp = ""
                        for j, part in enumerate(parts):
                            part = part.strip()
                            if not part:
                                continue

                            # ì‰¼í‘œ ë‹¤ì‹œ ì¶”ê°€ (ë§ˆì§€ë§‰ ì œì™¸)
                            if j < len(parts) - 1:
                                part_with_comma = part + ','
                            else:
                                part_with_comma = part

                            if len(temp + ' ' + part_with_comma) <= max_length and temp:
                                temp = temp + ' ' + part_with_comma
                            else:
                                if temp:
                                    segments.append(temp.strip())
                                temp = part_with_comma

                        if temp:
                            segments.append(temp.strip())
                    else:
                        # ì‰¼í‘œë„ ì—†ìœ¼ë©´ ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 
                        words = sentence.split()
                        temp = ""
                        for word in words:
                            if len(temp + ' ' + word) <= max_length and temp:
                                temp = temp + ' ' + word
                            else:
                                if temp:
                                    segments.append(temp.strip())
                                temp = word

                        if temp:
                            segments.append(temp.strip())

            return [seg for seg in segments if seg]  # ë¹ˆ ë¬¸ìì—´ ì œê±°

        # SRT íŒŒì¼ ì‘ì„±
        with open(output_srt_path, 'w', encoding='utf-8') as srt_file:
            current_time = 0.0  # ëˆ„ì  ì‹œê°„ (ì´ˆ)
            srt_index = 1  # ìë§‰ ë²ˆí˜¸

            for clip, audio_seg in zip(clips, audio_segments):
                # ì˜¤ë””ì˜¤ ê¸¸ì´ (ë°€ë¦¬ì´ˆ â†’ ì´ˆ)
                duration = len(audio_seg) / 1000.0

                # í…ìŠ¤íŠ¸ (SSML íƒœê·¸ ì œê±°)
                text = clip['text'].strip()
                if text.startswith('<speak>') and text.endswith('</speak>'):
                    text = text.replace('<speak>', '').replace('</speak>', '').strip()

                # í…ìŠ¤íŠ¸ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ë¶„í• 
                text_segments = split_text_smartly(text, max_chars)

                # ê° ì„¸ê·¸ë¨¼íŠ¸ì— ì‹œê°„ í• ë‹¹
                time_per_segment = duration / len(text_segments)

                for seg_idx, seg_text in enumerate(text_segments):
                    seg_start = current_time + (seg_idx * time_per_segment)
                    seg_end = seg_start + time_per_segment

                    # SRT í˜•ì‹: ë²ˆí˜¸, íƒ€ì„ìŠ¤íƒ¬í”„, í…ìŠ¤íŠ¸, ë¹ˆ ì¤„
                    srt_file.write(f"{srt_index}\n")
                    srt_file.write(f"{format_timestamp(seg_start)} --> {format_timestamp(seg_end)}\n")
                    srt_file.write(f"{seg_text}\n\n")

                    srt_index += 1

                # ë‹¤ìŒ í´ë¦½ì„ ìœ„í•´ ì‹œê°„ ëˆ„ì 
                current_time += duration

        if app:
            app.log_message(f"  âœ“ SRT íŒŒì¼ ìƒì„± ì™„ë£Œ: {output_srt_path}")
            app.log_message(f"  ğŸ“Š ì´ {srt_index - 1}ê°œì˜ ìë§‰ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±ë¨")

        return True

    except Exception as e:
        import traceback
        if app:
            app.log_message(f"  âŒ SRT ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            app.log_message(f"{traceback.format_exc()}")
        return False

def _render_chunk_worker(args):
    app, audio_chunk_path, job, chunk_index, is_batch = args
    try:
        y, sr = librosa.load(audio_chunk_path, sr=None, mono=True, dtype=np.float32)
        duration = librosa.get_duration(y=y, sr=sr)
        tab_ref = app.batch_process_tab if is_batch else app.video_maker_tab

        eq_settings = job['eq_settings']
        raw_style = eq_settings.get('style', 'ë§‰ëŒ€í˜•')

        # í•œê¸€ ìŠ¤íƒ€ì¼ëª… -> ë‚´ë¶€ ìŠ¤íƒ€ì¼ ì½”ë“œ ë§¤í•‘
        style_map = {
            'ë§‰ëŒ€í˜•': 'bar',
            'ë¯¸ëŸ¬ë§‰ëŒ€í˜•': 'mirror',
            'ì›í˜•': 'circular',
            'íŒŒí˜•': 'wave',
            'bar': 'bar',
            'mirror': 'mirror',
            'circular': 'circular',
            'wave': 'wave'
        }
        visualizer_style = style_map.get(raw_style, 'bar')

        # EQ í¬ê¸° ê³„ì‚°: ë°” ê°€ë¡œ + ê°„ê²© Ã— ë°”ê°¯ìˆ˜
        bar_width_px = eq_settings.get('barWidth', 20)  # ë°” 1ê°œ ê°€ë¡œ (px)
        bar_gap_px = eq_settings.get('barGap', 3)       # ë°” ê°„ê²© (px)
        n_bars = eq_settings.get('barCount', 24)        # ë°” ê°¯ìˆ˜
        n_bars = max(4, min(128, int(n_bars)))          # 4~128 ë²”ìœ„ë¡œ ì œí•œ

        eq_width = (bar_width_px + bar_gap_px) * n_bars   # EQ ì „ì²´ ê°€ë¡œ
        eq_height = eq_settings.get('height', 100)        # ë°” ì„¸ë¡œ ìµœëŒ€ ë†’ì´ (px)

        # ë¯¸ëŸ¬ë§‰ëŒ€í˜•ì€ ìœ„ì•„ë˜ ëŒ€ì¹­ì´ë¯€ë¡œ ë†’ì´ 2ë°°
        if visualizer_style == 'mirror':
            render_w, render_h = eq_width, eq_height * 2
        else:
            render_w, render_h = eq_width, eq_height

        # í¬ë¡œë§ˆí‚¤ ë°©ì‹: ë…¹ìƒ‰ ë°°ê²½ìœ¼ë¡œ ë Œë”ë§ í›„ íˆ¬ëª…ìœ¼ë¡œ ë³€í™˜
        # (matplotlibì˜ íˆ¬ëª… ë°°ê²½ì€ buffer_rgba()ì—ì„œ ì œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•ŠìŒ)
        CHROMA_KEY = (0, 255, 0)  # ìˆœìˆ˜ ë…¹ìƒ‰ ë°°ê²½

        fig = plt.Figure(figsize=(render_w / 100.0, render_h / 100.0), dpi=100, facecolor=(0, 1, 0, 1))  # ë…¹ìƒ‰ ë°°ê²½
        fig.subplots_adjust(left=0, right=1, top=1, bottom=0)
        canvas = FigureCanvasAgg(fig)
        ax = fig.add_axes([0, 0, 1, 1])
        ax.set_facecolor((0, 1, 0, 1))  # ë…¹ìƒ‰ ë°°ê²½
        ax.axis("off")
        ax.set_xlim(0, 1); ax.set_ylim(0, 1)

        # n_barsëŠ” ì´ë¯¸ ìœ„ì—ì„œ ì„¤ì •ë¨
        n_segs = 18  # side_bar ìŠ¤íƒ€ì¼ìš© ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜
        S = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, hop_length=512, n_mels=n_bars)
        
        # ì•ˆì „í•œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ì²˜ë¦¬ (ë¬´ìŒ êµ¬ê°„ ëŒ€ì‘)
        max_val = np.max(S)
        if max_val > 0:
            S_db = librosa.power_to_db(S, ref=max_val)
        else:
            S_db = np.zeros_like(S)
        
        smin, smax = float(np.min(S_db)), float(np.max(S_db))
        if smax - smin < 1e-6:
            smin, smax = -80.0, 0.0

        # ì‚¬ìš©ì ì„¤ì • ìƒ‰ìƒ ì‚¬ìš© (color1 -> color2 ê·¸ë¼ë°ì´ì…˜)
        # ë¯¸ë¦¬ë³´ê¸°ì™€ ë™ì¼: ê° ë°” ë‚´ë¶€ì—ì„œ ì•„ë˜(color1) -> ìœ„(color2) ì„¸ë¡œ ê·¸ë¼ë°ì´ì…˜
        color1 = eq_settings.get('color1', '#667eea')
        color2 = eq_settings.get('color2', '#764ba2')
        app.log_message(f"  EQ ìƒ‰ìƒ ì„¤ì •: color1={color1}, color2={color2}")

        def hex_to_rgb(hex_color):
            hex_color = hex_color.lstrip('#')
            return tuple(int(hex_color[i:i+2], 16) / 255.0 for i in (0, 2, 4))

        rgb1 = hex_to_rgb(color1)  # ì•„ë˜ìª½ ìƒ‰ìƒ
        rgb2 = hex_to_rgb(color2)  # ìœ„ìª½ ìƒ‰ìƒ

        # ì„¸ë¡œ ê·¸ë¼ë°ì´ì…˜ì„ ìœ„í•œ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜ (ê° ë°”ë¥¼ ì—¬ëŸ¬ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ”)
        n_gradient_segments = 20
        
        if visualizer_style == 'bar':
            # ë§‰ëŒ€í˜• ìŠ¤íƒ€ì¼ - ë¼ìš´ë“œ ì²˜ë¦¬ëœ ë§‰ëŒ€ (ë¯¸ë¦¬ë³´ê¸°ì™€ ë™ì¼)
            from matplotlib.patches import FancyBboxPatch
            bars_patches = []

            bar_w_ratio = bar_width_px / render_w
            gap_ratio = bar_gap_px / render_w
            bar_slot = bar_w_ratio + gap_ratio
            max_height = 1.0
            border_radius = bar_w_ratio * 0.15  # ë¼ìš´ë“œ ë°˜ê²½

            # ìˆ˜í‰ ê·¸ë¼ë°ì´ì…˜ ìƒ‰ìƒ (ì™¼ìª½ -> ì˜¤ë¥¸ìª½)
            for b in range(n_bars):
                x0 = b * bar_slot
                t = b / max(n_bars - 1, 1)
                col = (
                    rgb1[0] + (rgb2[0] - rgb1[0]) * t,
                    rgb1[1] + (rgb2[1] - rgb1[1]) * t,
                    rgb1[2] + (rgb2[2] - rgb1[2]) * t
                )
                # ë¼ìš´ë“œ ì²˜ë¦¬ëœ ë§‰ëŒ€
                bar_patch = FancyBboxPatch(
                    (x0, 0), bar_w_ratio, 0.01,
                    boxstyle=f"round,pad=0,rounding_size={border_radius}",
                    facecolor=(*col, 1.0), edgecolor='none'
                )
                ax.add_patch(bar_patch)
                bars_patches.append((bar_patch, col))

            prev = np.zeros(n_bars, dtype=np.float32)
            decay = 0.15

            def update_bar(i):
                if app.cancel_event.is_set(): raise StopIteration
                t = i / fps
                idx = min(int(t * sr / 512), S_db.shape[1] - 1)
                cur = np.clip((S_db[:, idx] - smin) / (smax - smin + 1e-6), 0, 1)
                levels = np.maximum(cur, prev - decay); prev[:] = levels

                for b in range(n_bars):
                    bar_height = max(0.01, levels[b] * max_height * 0.9)
                    x0 = b * bar_slot
                    patch, col = bars_patches[b]
                    patch.set_bounds(x0, 0, bar_w_ratio, bar_height)
                    patch.set_alpha(min(0.5 + levels[b] * 0.5, 1.0))

                return [p[0] for p in bars_patches]

            update_func = update_bar

        elif visualizer_style == 'mirror':
            # ë¯¸ëŸ¬ë§‰ëŒ€í˜• ìŠ¤íƒ€ì¼ - ìœ„ì•„ë˜ ëŒ€ì¹­ (ì¤‘ì•™ì—ì„œ ìœ„ì•„ë˜ë¡œ ë»—ì–´ë‚˜ê°)
            # FancyBboxPatchë¡œ ë¼ìš´ë“œ ì²˜ë¦¬ëœ ë§‰ëŒ€ ì‚¬ìš©
            from matplotlib.patches import FancyBboxPatch
            bars_top = []
            bars_bottom = []

            bar_w_ratio = bar_width_px / render_w
            gap_ratio = bar_gap_px / render_w
            bar_slot = bar_w_ratio + gap_ratio
            half_height = 0.5  # ìœ„/ì•„ë˜ ê°ê° ì ˆë°˜ì”© ì°¨ì§€
            border_radius = bar_w_ratio * 0.15  # ë¼ìš´ë“œ í¬ê¸°

            for b in range(n_bars):
                x0 = b * bar_slot
                # ê°€ë¡œ ê·¸ë¼ë°ì´ì…˜ ìƒ‰ìƒ (ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½)
                t = b / max(n_bars - 1, 1)
                col = (
                    rgb1[0] + (rgb2[0] - rgb1[0]) * t,
                    rgb1[1] + (rgb2[1] - rgb1[1]) * t,
                    rgb1[2] + (rgb2[2] - rgb1[2]) * t
                )
                # ìƒë‹¨ ë°” (0.5ì—ì„œ ìœ„ë¡œ)
                bar_top = FancyBboxPatch(
                    (x0, 0.5), bar_w_ratio, 0.01,
                    boxstyle=f"round,pad=0,rounding_size={border_radius}",
                    facecolor=(*col, 1.0), edgecolor='none'
                )
                ax.add_patch(bar_top)
                bars_top.append((bar_top, col))

                # í•˜ë‹¨ ë°” (0.5ì—ì„œ ì•„ë˜ë¡œ)
                bar_bottom = FancyBboxPatch(
                    (x0, 0.49), bar_w_ratio, 0.01,
                    boxstyle=f"round,pad=0,rounding_size={border_radius}",
                    facecolor=(*col, 1.0), edgecolor='none'
                )
                ax.add_patch(bar_bottom)
                bars_bottom.append((bar_bottom, col))

            prev = np.zeros(n_bars, dtype=np.float32)
            decay = 0.08

            def update_mirror(i):
                if app.cancel_event.is_set(): raise StopIteration
                t = i / fps
                idx = min(int(t * sr / 512), S_db.shape[1] - 1)
                cur = np.clip((S_db[:, idx] - smin) / (smax - smin + 1e-6), 0, 1)
                levels = np.maximum(cur, prev - decay); prev[:] = levels

                for b in range(n_bars):
                    bar_height = max(0.01, levels[b] * half_height * 0.9)
                    x0 = b * bar_slot
                    patch_top, col = bars_top[b]
                    patch_bottom, _ = bars_bottom[b]
                    # ìƒë‹¨: ì¤‘ì•™ì—ì„œ ìœ„ë¡œ
                    patch_top.set_bounds(x0, 0.5, bar_w_ratio, bar_height)
                    patch_top.set_alpha(min(0.5 + levels[b] * 0.5, 1.0))
                    # í•˜ë‹¨: ì¤‘ì•™ì—ì„œ ì•„ë˜ë¡œ (yì¢Œí‘œë¥¼ ì•„ë˜ë¡œ)
                    patch_bottom.set_bounds(x0, 0.5 - bar_height, bar_w_ratio, bar_height)
                    patch_bottom.set_alpha(min(0.5 + levels[b] * 0.5, 1.0))

                return [p[0] for p in bars_top] + [p[0] for p in bars_bottom]

            update_func = update_mirror

        elif visualizer_style == 'side_bar':
            # ì¢Œìš° ì¸¡ë©´ ë§‰ëŒ€í˜• ìŠ¤íƒ€ì¼ - ê°œë³„ íŒ¨ì¹˜ ì‚¬ìš©
            patches = []
            patch_colors = []  # ê° íŒ¨ì¹˜ì˜ ê¸°ë³¸ ìƒ‰ìƒ ì €ì¥
            bar_gap, seg_gap = 0.5, 0.35
            bar_slot, bar_w = 1/n_bars, 1/n_bars*(1-bar_gap)
            seg_slot_w, seg_w = 1/n_segs, 1/n_segs*(1-seg_gap)

            for b in range(n_bars):
                # ê·¸ë¼ë°ì´ì…˜ ìƒ‰ìƒ ê³„ì‚°
                t = b / max(n_bars - 1, 1)
                col = (
                    rgb1[0] + (rgb2[0] - rgb1[0]) * t,
                    rgb1[1] + (rgb2[1] - rgb1[1]) * t,
                    rgb1[2] + (rgb2[2] - rgb1[2]) * t
                )
                y0 = b*bar_slot + (bar_slot-bar_w)/2
                for s in range(n_segs):
                    # ì™¼ìª½ ë§‰ëŒ€
                    x0_left = 0.5 - (s*seg_slot_w/2 + (seg_slot_w/2 - seg_w/2)/2) - seg_w/2
                    # ì˜¤ë¥¸ìª½ ë§‰ëŒ€
                    x0_right = 0.5 + s*seg_slot_w/2 + (seg_slot_w/2 - seg_w/2)/2
                    rect_left = Rectangle((x0_left, y0), seg_w/2, bar_w, facecolor=(*col, 0), edgecolor='none')
                    rect_right = Rectangle((x0_right, y0), seg_w/2, bar_w, facecolor=(*col, 0), edgecolor='none')
                    ax.add_patch(rect_left)
                    ax.add_patch(rect_right)
                    patches.extend([rect_left, rect_right])
                    patch_colors.extend([col, col])

            prev = np.zeros(n_bars, dtype=np.float32)
            decay = 0.08

            def update_side_bar(i):
                if app.cancel_event.is_set(): raise StopIteration
                t = i / fps; idx = min(int(t * sr / 512), S_db.shape[1] - 1)
                cur = np.clip((S_db[:, idx] - smin) / (smax - smin + 1e-6), 0, 1)
                levels = np.maximum(cur, prev - decay); prev[:] = levels
                on = (levels * n_segs + 1e-6).astype(int)
                k = 0
                for b in range(n_bars):
                    nb = on[b]
                    for s in range(n_segs):
                        alpha = 1.0 if s < nb else 0.0
                        patches[k].set_facecolor((*patch_colors[k], alpha))
                        patches[k+1].set_facecolor((*patch_colors[k+1], alpha))
                        k += 2
                app.update_progress(f"ì˜ìƒ ë Œë”ë§ ì¤‘: {i + 1}/{total_frames}", 40 + ((i + 1) / total_frames * 45), is_batch=is_batch)
                return patches

            update_func = update_side_bar

        elif visualizer_style == 'spectrum':
            # ìŠ¤í™íŠ¸ëŸ¼ ìŠ¤íƒ€ì¼ (í•˜ë‹¨ ê°€ë¡œ ë§‰ëŒ€)
            # PatchCollection ëŒ€ì‹  ê°œë³„ Rectangleì„ axì— ì§ì ‘ ì¶”ê°€
            patches = []
            # ë°” ê°€ë¡œ = bar_width_px, ê°„ê²© = bar_gap_px (í”½ì…€ ê¸°ì¤€)
            bar_w_ratio = bar_width_px / render_w
            gap_ratio = bar_gap_px / render_w
            bar_slot = bar_w_ratio + gap_ratio
            max_height = 0.3  # ìµœëŒ€ ë†’ì´ (í™”ë©´ í•˜ë‹¨ì—ì„œ 30%)

            for b in range(n_bars):
                # ê·¸ë¼ë°ì´ì…˜ ìƒ‰ìƒ ê³„ì‚°
                t = b / max(n_bars - 1, 1)
                col = (
                    rgb1[0] + (rgb2[0] - rgb1[0]) * t,
                    rgb1[1] + (rgb2[1] - rgb1[1]) * t,
                    rgb1[2] + (rgb2[2] - rgb1[2]) * t
                )
                x0 = b * bar_slot
                rect = Rectangle((x0, 0), bar_w_ratio, 0.01, facecolor=(*col, 0.8), edgecolor='none')
                ax.add_patch(rect)
                patches.append(rect)

            prev = np.zeros(n_bars, dtype=np.float32)
            decay = 0.08

            def update_spectrum(i):
                if app.cancel_event.is_set(): raise StopIteration
                t = i / fps; idx = min(int(t * sr / 512), S_db.shape[1] - 1)
                cur = np.clip((S_db[:, idx] - smin) / (smax - smin + 1e-6), 0, 1)
                levels = np.maximum(cur, prev - decay); prev[:] = levels

                for b in range(n_bars):
                    height = max(levels[b] * max_height, 0.01)
                    patches[b].set_height(height)

                return patches

            update_func = update_spectrum

        elif visualizer_style == 'circular':
            # ì›í˜• (ì  ìŠ¤íƒ€ì¼) - í•œ ì¤„ë¡œ ì ë“¤ ë°°ì¹˜ (ë¯¸ë¦¬ë³´ê¸°ì™€ ë™ì¼)
            from matplotlib.patches import Circle
            dots = []

            bar_w_ratio = bar_width_px / render_w
            gap_ratio = bar_gap_px / render_w
            bar_slot = bar_w_ratio + gap_ratio
            dot_size_base = bar_w_ratio * 0.4  # ì  ê¸°ë³¸ í¬ê¸°

            for b in range(n_bars):
                # ê·¸ë¼ë°ì´ì…˜ ìƒ‰ìƒ ê³„ì‚°
                t = b / max(n_bars - 1, 1)
                col = (
                    rgb1[0] + (rgb2[0] - rgb1[0]) * t,
                    rgb1[1] + (rgb2[1] - rgb1[1]) * t,
                    rgb1[2] + (rgb2[2] - rgb1[2]) * t
                )
                x0 = b * bar_slot + bar_w_ratio / 2
                dot = Circle((x0, 0.5), dot_size_base, facecolor=(*col, 0), edgecolor='none')
                ax.add_patch(dot)
                dots.append(dot)

            prev = np.zeros(n_bars, dtype=np.float32)
            decay = 0.08

            def update_circular(i):
                if app.cancel_event.is_set(): raise StopIteration
                t = i / fps; idx = min(int(t * sr / 512), S_db.shape[1] - 1)
                cur = np.clip((S_db[:, idx] - smin) / (smax - smin + 1e-6), 0, 1)
                levels = np.maximum(cur, prev - decay); prev[:] = levels

                for b in range(n_bars):
                    # ì  í¬ê¸°ì™€ íˆ¬ëª…ë„ë¥¼ ì˜¤ë””ì˜¤ ë ˆë²¨ì— ë”°ë¼ ì¡°ì ˆ
                    size = dot_size_base * (0.3 + levels[b] * 0.7)
                    dots[b].set_radius(size)
                    dots[b].set_alpha(min(0.3 + levels[b] * 0.7, 1.0))

                app.update_progress(f"ì˜ìƒ ë Œë”ë§ ì¤‘: {i + 1}/{total_frames}", 40 + ((i + 1) / total_frames * 45), is_batch=is_batch)
                return dots

            update_func = update_circular

        elif visualizer_style == 'wave':
            # íŒŒí˜• ìŠ¤íƒ€ì¼ - ì‚¬ì¸íŒŒ í˜•íƒœ ë§‰ëŒ€ (ë¼ìš´ë“œ ì²˜ë¦¬)
            from matplotlib.patches import FancyBboxPatch
            bars_patches = []

            bar_w_ratio = bar_width_px / render_w
            gap_ratio = bar_gap_px / render_w
            bar_slot = bar_w_ratio + gap_ratio
            max_height = 1.0
            border_radius = bar_w_ratio * 0.15  # ë¼ìš´ë“œ í¬ê¸°

            for b in range(n_bars):
                x0 = b * bar_slot
                # ê°€ë¡œ ê·¸ë¼ë°ì´ì…˜ ìƒ‰ìƒ (ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½)
                t = b / max(n_bars - 1, 1)
                col = (
                    rgb1[0] + (rgb2[0] - rgb1[0]) * t,
                    rgb1[1] + (rgb2[1] - rgb1[1]) * t,
                    rgb1[2] + (rgb2[2] - rgb1[2]) * t
                )
                bar_patch = FancyBboxPatch(
                    (x0, 0), bar_w_ratio, 0.01,
                    boxstyle=f"round,pad=0,rounding_size={border_radius}",
                    facecolor=(*col, 1.0), edgecolor='none'
                )
                ax.add_patch(bar_patch)
                bars_patches.append((bar_patch, col))

            prev = np.zeros(n_bars, dtype=np.float32)
            decay = 0.15

            def update_wave(i):
                if app.cancel_event.is_set(): raise StopIteration
                t = i / fps; idx = min(int(t * sr / 512), S_db.shape[1] - 1)
                cur = np.clip((S_db[:, idx] - smin) / (smax - smin + 1e-6), 0, 1)
                levels = np.maximum(cur, prev - decay); prev[:] = levels

                for b in range(n_bars):
                    # íŒŒí˜• íš¨ê³¼ ì¶”ê°€ (ì‚¬ì¸íŒŒë¡œ ë†’ì´ ë³€ì¡°)
                    wave_factor = np.sin(b * 0.3 + i * 0.1) * 0.2
                    level = max(0.01, levels[b] * (0.8 + wave_factor) * max_height * 0.9)
                    x0 = b * bar_slot
                    patch, col = bars_patches[b]
                    patch.set_bounds(x0, 0, bar_w_ratio, level)
                    patch.set_alpha(min(0.5 + levels[b] * 0.5, 1.0))

                app.update_progress(f"ì˜ìƒ ë Œë”ë§ ì¤‘: {i + 1}/{total_frames}", 40 + ((i + 1) / total_frames * 45), is_batch=is_batch)
                return [p[0] for p in bars_patches]

            update_func = update_wave
        
        # FPSëŠ” jobì˜ eq_settingsì—ì„œ ê°€ì ¸ì˜¤ê¸°
        fps = eq_settings.get('fps', 20)  # ê¸°ë³¸ê°’ 20
        total_frames = max(1, int(duration * fps))
        output_path = os.path.join(TEMP_DIR, f"vis_chunk_{chunk_index}.mov")

        # PNG ì‹œí€€ìŠ¤ ë°©ì‹ìœ¼ë¡œ EQ ë Œë”ë§ (íˆ¬ëª… ë°°ê²½ ë³´ì¥)
        app.log_message(f"  EQ ì˜ìƒ ë Œë”ë§ ì‹œì‘: {total_frames}í”„ë ˆì„, {fps}fps")

        from PIL import Image as PILImage
        import subprocess

        # ì„ì‹œ í”„ë ˆì„ í´ë” ìƒì„±
        frames_dir = os.path.join(TEMP_DIR, f"eq_frames_{chunk_index}_{uuid.uuid4().hex[:8]}")
        os.makedirs(frames_dir, exist_ok=True)

        try:
            # í”„ë ˆì„ë³„ë¡œ PNG ì €ì¥
            app.log_message(f"  PNG í”„ë ˆì„ í´ë”: {frames_dir}")

            for i in range(total_frames):
                if app.cancel_event.is_set():
                    return None

                # ì—…ë°ì´íŠ¸ í•¨ìˆ˜ í˜¸ì¶œ
                update_func(i)

                # canvasì—ì„œ RGBA ë²„í¼ ì§ì ‘ ì¶”ì¶œ
                canvas.draw()
                buf = canvas.buffer_rgba()
                rgba_array = np.asarray(buf).copy()  # copy()ë¡œ ë²„í¼ ê³ ì •

                # í¬ë¡œë§ˆí‚¤ ë…¹ìƒ‰(0,255,0)ì„ íˆ¬ëª…ìœ¼ë¡œ ë³€í™˜
                # ë…¹ìƒ‰ í”½ì…€ ì°¾ê¸° (R<10, G>240, B<10)
                green_mask = (rgba_array[:,:,0] < 10) & (rgba_array[:,:,1] > 240) & (rgba_array[:,:,2] < 10)
                rgba_array[green_mask, 3] = 0  # ë…¹ìƒ‰ í”½ì…€ì˜ ì•ŒíŒŒë¥¼ 0ìœ¼ë¡œ

                # ì²« í”„ë ˆì„ ë””ë²„ê·¸ ì •ë³´
                non_green_count = np.sum(~green_mask)
                if i == 0:
                    app.log_message(f"  ì²« í”„ë ˆì„ RGBA: shape={rgba_array.shape}, ë¹„ë…¹ìƒ‰í”½ì…€ìˆ˜={non_green_count}, A_max={rgba_array[:,:,3].max()}, A_min={rgba_array[:,:,3].min()}")

                # PILë¡œ RGBA ì´ë¯¸ì§€ ì €ì¥
                pil_img = PILImage.fromarray(rgba_array, 'RGBA')
                frame_path = os.path.join(frames_dir, f"frame_{i:05d}.png")
                pil_img.save(frame_path, 'PNG')

                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (20í”„ë ˆì„ë§ˆë‹¤)
                if i % 20 == 0:
                    progress = 40 + ((i + 1) / total_frames * 40)
                    app.update_progress(f"EQ ë Œë”ë§ ì¤‘: {i + 1}/{total_frames}", progress, is_batch=is_batch)

            plt.close(fig)

            # PNG íŒŒì¼ ìˆ˜ í™•ì¸
            import glob
            png_files = glob.glob(os.path.join(frames_dir, '*.png'))
            app.log_message(f"  ìƒì„±ëœ PNG íŒŒì¼ ìˆ˜: {len(png_files)}")

            # FFmpegë¡œ PNG ì‹œí€€ìŠ¤ë¥¼ MOVë¡œ ë³€í™˜ (íˆ¬ëª… ë°°ê²½ ìœ ì§€)
            app.log_message(f"  PNG ì‹œí€€ìŠ¤ë¥¼ MOVë¡œ ë³€í™˜ ì¤‘...")
            ffmpeg_cmd = [
                'ffmpeg', '-y',
                '-framerate', str(fps),
                '-i', os.path.join(frames_dir, 'frame_%05d.png'),
                '-c:v', 'qtrle',  # QuickTime Animation codec
                '-pix_fmt', 'argb',  # ARGB for transparency
                output_path
            ]

            result = subprocess.run(
                ffmpeg_cmd,
                capture_output=True,
                text=True,
                creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0
            )

            if result.returncode != 0:
                app.log_message(f"  FFmpeg ì˜¤ë¥˜: {result.stderr[:500]}")
                raise RuntimeError(f"FFmpeg ë³€í™˜ ì‹¤íŒ¨: {result.stderr[:200]}")

            app.log_message(f"  âœ“ EQ ì˜ìƒ ìƒì„± ì™„ë£Œ: {output_path}")
            return output_path

        finally:
            # ì„ì‹œ í”„ë ˆì„ í´ë” ì‚­ì œ
            import shutil
            if os.path.exists(frames_dir):
                try:
                    shutil.rmtree(frames_dir)
                except:
                    pass
    except Exception as e:
        import traceback
        app.log_message(f"ì˜¤ë¥˜: ë¹„ì£¼ì–¼ë¼ì´ì € ì²­í¬ {chunk_index} ë Œë”ë§ ì‹¤íŒ¨ - {e}\n{traceback.format_exc()}")
        return None

def render_visualizer_video(app, audio_path, job, is_batch=False):
    app.log_message("ë¹„ì£¼ì–¼ë¼ì´ì € ë Œë”ë§ ì‹œì‘..."); args = (app, audio_path, job, 0, is_batch); return _render_chunk_worker(args)

def _execute_single_video_job(app, job, is_batch=False):
    temp_files = []
    try:
        app.log_message(f"\n[ë””ë²„ê·¸] _execute_single_video_job ì‹œì‘")
        app.log_message(f"  - is_batch: {is_batch}")
        app.log_message(f"  - image_path ì¡´ì¬: {'image_path' in job}")
        if 'image_path' in job:
            app.log_message(f"  - image_path ÃªÂ°': {job['image_path']}")
            app.log_message(f"  - image_path íŒŒì¼ ì¡´ì¬: {os.path.exists(job['image_path'])}")
        app.log_message(f"  - eq_settings ì¡´ì¬: {'eq_settings' in job}")
        if 'eq_settings' in job:
            app.log_message(f"  - eq_settings ÃªÂ°': {job['eq_settings']}")
        
        app.update_progress("ì˜¤ë””ì˜¤ ìƒì„± ì‹œì‘...", 5, is_batch)
        combined_audio = AudioSegment.empty()
        audio_segments = []  # ê° í´ë¦½ë³„ ì˜¤ë””ì˜¤ ì €ì¥ (SRT ìƒì„±ìš©)
        if is_batch:
            # ë°°ì¹˜ ëª¨ë“œ: ëŒ€ë³¸ íŒŒì¼ì„ ì½ì–´ì„œ [ìºë¦­í„°ëª…] íŒ¨í„´ìœ¼ë¡œ íŒŒì‹±
            script_text = utils.read_script_file(job['scriptPath'])
            clips = []
            current_character = 'ë‚˜ë ˆì´ì…˜'
            current_lines = []

            for line in script_text.split('\n'):
                line = line.strip()
                if not line:
                    continue

                # [ìºë¦­í„°ëª…] íŒ¨í„´ ì²´í¬
                import re
                char_match = re.match(r'^\[([^\]]+)\]\s*(.*)', line)
                if char_match:
                    # ì´ì „ ìºë¦­í„°ì˜ ëŒ€ì‚¬ê°€ ìˆìœ¼ë©´ clipsì— ì¶”ê°€
                    if current_lines:
                        text = ' '.join(current_lines)
                        clips.append({
                            "character": current_character,
                            "text": text,
                            "is_ssml": False
                        })
                        current_lines = []

                    # ìƒˆ ìºë¦­í„° ì‹œì‘
                    current_character = char_match.group(1).strip()
                    remaining_text = char_match.group(2).strip()
                    if remaining_text:
                        current_lines.append(remaining_text)
                else:
                    # ìºë¦­í„° ì§€ì •ì´ ì—†ëŠ” ë¼ì¸ì€ í˜„ì¬ ìºë¦­í„°ì— ì¶”ê°€
                    current_lines.append(line)

            # ë§ˆì§€ë§‰ ìºë¦­í„°ì˜ ëŒ€ì‚¬ ì¶”ê°€
            if current_lines:
                text = ' '.join(current_lines)
                clips.append({
                    "character": current_character,
                    "text": text,
                    "is_ssml": False
                })

            app.log_message(f"[ë°°ì¹˜] ëŒ€ë³¸ íŒŒì‹± ì™„ë£Œ: {len(clips)}ê°œ í´ë¦½")
        else: clips = job['clips']

        for i, clip in enumerate(clips):
            if app.cancel_event.is_set(): return False

            char = clip['character']
            text_preview = clip['text'][:50] + "..." if len(clip['text']) > 50 else clip['text']
            app.log_message(f"\n[í´ë¦½ {i+1}/{len(clips)}] '{char}' ì²˜ë¦¬ ì¤‘...")
            app.log_message(f"  í…ìŠ¤íŠ¸: {text_preview}")

            app.update_progress(f"ìŒì„± ìƒì„± ì¤‘ ({i+1}/{len(clips)})...", 5 + (i/len(clips)*35), is_batch)

            # narration_settingsì—ì„œ ìºë¦­í„° ì„¤ì • ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            if char in job['narration_settings']:
                w = job['narration_settings'][char]
            else:
                # ê¸°ë³¸ ìŒì„± ì„¤ì • ì‚¬ìš©
                app.log_message(f"  ê²½ê³ : '{char}' ìŒì„± ì„¤ì •ì´ ì—†ì–´ ê¸°ë³¸ê°’ ì‚¬ìš©")
                w = {
                    'voice': 'ko-KR-Wavenet-A',
                    'speed': 1.0,
                    'pitch': 0.0
                }

            # voice í•„ë“œê°€ ì´ë¯¸ API í˜•ì‹ì¸ì§€ í™•ì¸ (Eel ë²„ì „ í˜¸í™˜ì„±)
            if w['voice'].startswith(('ko-', 'en-', 'ja-', 'es-', 'fr-', 'de-', 'it-', 'pt-', 'ru-', 'zh-', 'hi-', 'ar-')):
                # ì´ë¯¸ API í˜•ì‹ (ì˜ˆ: ko-KR-Standard-A)
                api_voice = w['voice']
            else:
                # ë‚´ë¶€ í˜•ì‹ (ì˜ˆ: ì—¬ì„±_A) -> API í˜•ì‹ìœ¼ë¡œ ë³€í™˜ í•„ìš” (Tkinter ë²„ì „)
                api_voice = next((name for name, gender in config.LANG_VOICE_GROUPS.get(w['lang'], {}).get(w['group'], {}).items() if app.video_maker_tab._format_voice_name_internal(name, gender) == w['voice']), None)
                if not api_voice: raise ValueError(f"API ìŒì„±ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {w['voice']}")

            audio_bytes = synthesize_tts_bytes(job['api_key_profile'], clip['text'], api_voice, w['speed'], w['pitch'], w.get('volumeGain', 0), clip.get('is_ssml', False), app=app, pause_after_ms=w.get('pauseAfter', 0))
            audio_seg = AudioSegment.from_mp3(io.BytesIO(audio_bytes))
            audio_segments.append(audio_seg)  # SRT ìƒì„±ìš© ì €ì¥
            combined_audio += audio_seg
            app.log_message(f"  âœ“ ì™„ë£Œ!")

        if app.cancel_event.is_set(): return False
        audio_path = os.path.join(TEMP_DIR, f"temp_audio_{job.get('id', uuid.uuid4())}.mp3")
        temp_files.append(audio_path); combined_audio.export(audio_path, format="mp3")
        app.log_message(f"\n[ë””ë²„ê·¸] ì˜¤ë””ì˜¤ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {audio_path}")

        # EQ í™œì„±í™” ì—¬ë¶€ í™•ì¸
        eq_settings = job.get('eq_settings', {})
        eq_enabled = eq_settings.get('enabled', True)
        app.log_message(f"[ë””ë²„ê·¸] EQ í™œì„±í™”: {eq_enabled}")

        vis_path = None
        if eq_enabled:
            app.update_progress("ë¹„ì£¼ì–¼ë¼ì´ì € ë Œë”ë§...", 40, is_batch)
            app.log_message(f"[ë””ë²„ê·¸] ë¹„ì£¼ì–¼ë¼ì´ì € ë Œë”ë§ ì‹œì‘...")
            vis_path = render_visualizer_video(app, audio_path, job, is_batch)
            if not vis_path:
                app.log_message(f"[ì˜¤ë¥˜] ë¹„ì£¼ì–¼ë¼ì´ì € ë Œë”ë§ ì‹¤íŒ¨ - vis_pathê°€ Noneì…ë‹ˆë‹¤")
                raise RuntimeError("ë¹„ì£¼ì–¼ë¼ì´ì € ë Œë”ë§ ì‹¤íŒ¨")
            app.log_message(f"[ë””ë²„ê·¸] ë¹„ì£¼ì–¼ë¼ì´ì € ë Œë”ë§ ì™„ë£Œ: {vis_path}")
            temp_files.append(vis_path)
        else:
            app.log_message(f"[ë””ë²„ê·¸] EQ ë¹„í™œì„±í™” - ë¹„ì£¼ì–¼ë¼ì´ì € ë Œë”ë§ ê±´ë„ˆëœ€")
            app.update_progress("ì˜ìƒ ì¤€ë¹„ ì¤‘...", 40, is_batch)

        if app.cancel_event.is_set(): return False

        app.update_progress("ìµœì¢… ì˜ìƒ ê²°í•© ì¤‘...", 85, is_batch)
        app.log_message(f"[ë””ë²„ê·¸] ì˜ìƒ ê²°í•© ì‹œì‘...")
        app.log_message(f"  - image_path: {job.get('image_path', 'None')}")

        # ì¶œë ¥ í•´ìƒë„ ê°€ì ¸ì˜¤ê¸°
        output_resolution = eq_settings.get('resolution', '1920x1080')
        target_w, target_h = map(int, output_resolution.split('x'))
        app.log_message(f"  - ì¶œë ¥ í•´ìƒë„: {target_w}x{target_h}")

        # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸° í™•ì¸
        with Image.open(job['image_path']) as img:
            orig_w, orig_h = img.size
        app.log_message(f"  - ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°: {orig_w}x{orig_h}")

        # ì¶œë ¥ í•´ìƒë„ë¥¼ ìµœì¢… ì˜ìƒ í¬ê¸°ë¡œ ì‚¬ìš©
        img_w, img_h = target_w, target_h
        app.log_message(f"  - ìµœì¢… ì¶œë ¥ í¬ê¸°: {img_w}x{img_h}")

        # EQ í™œì„±í™” ì—¬ë¶€ì— ë”°ë¼ ë‹¤ë¥¸ ì²˜ë¦¬
        if eq_enabled and vis_path:
            # RoyStudio ë°©ì‹: VideoFileClipìœ¼ë¡œ íˆ¬ëª…ë„ ìˆëŠ” MOV íŒŒì¼ ë¡œë“œ
            # ë¨¼ì € MOV íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not os.path.exists(vis_path):
                app.log_message(f"[ì˜¤ë¥˜] EQ MOV íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {vis_path}")
                raise RuntimeError(f"EQ MOV íŒŒì¼ ì—†ìŒ: {vis_path}")

            vis_file_size = os.path.getsize(vis_path)
            app.log_message(f"[ë””ë²„ê·¸] EQ MOV íŒŒì¼ í¬ê¸°: {vis_file_size / 1024:.1f} KB")

            with VideoFileClip(vis_path, has_mask=True) as vis_clip, \
                 AudioFileClip(audio_path) as audio_clip, \
                 ImageClip(job['image_path'], duration=audio_clip.duration) as bg_clip_orig:

                # ë°°ê²½ ì´ë¯¸ì§€ë¥¼ ì¶œë ¥ í•´ìƒë„ì— ë§ê²Œ ë¦¬ì‚¬ì´ì¦ˆ
                if bg_clip_orig.size != (img_w, img_h):
                    bg_clip = bg_clip_orig.resized((img_w, img_h))
                    app.log_message(f"[ë””ë²„ê·¸] ë°°ê²½ ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ: {bg_clip_orig.size} -> {bg_clip.size}")
                else:
                    bg_clip = bg_clip_orig

                app.log_message(f"[ë””ë²„ê·¸] í´ë¦½ ë¡œë“œ ì™„ë£Œ:")
                app.log_message(f"  - vis_clip í¬ê¸°: {vis_clip.size}")
                app.log_message(f"  - vis_clip ë§ˆìŠ¤í¬: {vis_clip.mask}")
                app.log_message(f"  - vis_clip duration: {vis_clip.duration}ì´ˆ")
                app.log_message(f"  - audio_clip ê¸¸ì´: {audio_clip.duration}ì´ˆ")
                app.log_message(f"  - bg_clip í¬ê¸°: {bg_clip.size}, ê¸¸ì´: {bg_clip.duration}ì´ˆ")

                # EQ í´ë¦½ ì²« í”„ë ˆì„ í™•ì¸
                if vis_clip.duration > 0:
                    test_frame = vis_clip.get_frame(0)
                    app.log_message(f"  - vis_clip ì²« í”„ë ˆì„: shape={test_frame.shape}, dtype={test_frame.dtype}, max={test_frame.max()}, min={test_frame.min()}")

                # ë§ˆìŠ¤í¬ê°€ ì—†ìœ¼ë©´ ê²½ê³  (EQê°€ ê²€ì€ ë°°ê²½ìœ¼ë¡œ ë³´ì¼ ìˆ˜ ìˆìŒ)
                if vis_clip.mask is None:
                    app.log_message(f"  âš ï¸ ê²½ê³ : vis_clipì— ë§ˆìŠ¤í¬(íˆ¬ëª…ë„)ê°€ ì—†ìŠµë‹ˆë‹¤. EQê°€ ê²€ì€ ë°°ê²½ê³¼ í•¨ê»˜ í‘œì‹œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

                final_bg = bg_clip
                # ë°ê¸° ì¡°ì ˆ (Tkinter ë²„ì „ê³¼ Eel ë²„ì „ ëª¨ë‘ í˜¸í™˜)
                brightness_val = 100.0  # ê¸°ë³¸ê°’
                if not is_batch:
                    # Tkinter ë²„ì „ (brightness_var ì‚¬ìš©)
                    if hasattr(app, 'video_maker_tab') and hasattr(app.video_maker_tab, 'brightness_var'):
                        brightness_val = app.video_maker_tab.brightness_var.get()
                    # Eel ë²„ì „ (eq_settingsì—ì„œ brightness ê°€ì ¸ì˜¤ê¸°)
                    elif 'eq_settings' in job and 'brightness' in job['eq_settings']:
                        brightness_val = float(job['eq_settings']['brightness'])

                    if brightness_val != 100.0:
                        base_brightness = brightness_val / 100.0; oscillation = 0.1; period = 10
                        def brightness_func(t): return base_brightness + oscillation * math.sin(2 * math.pi * t / period)
                        final_bg = final_bg.fl(lambda gf, t: (gf(t) * brightness_func(t)).clip(0,255).astype('uint8'))

                app.log_message(f"[ë””ë²„ê·¸] EQ ì„¤ì •: {eq_settings}")

                # EQ í¬ê¸° (í”½ì…€ ê°’ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
                eq_w_pixels = eq_settings.get('width', 800)

                # EQ ìœ„ì¹˜ (ì¤‘ì•™ ê¸°ì¤€ì , í”½ì…€ ê°’)
                center_x = eq_settings.get('x', img_w // 2)
                center_y = eq_settings.get('y', img_h // 2)
                pos_x = center_x - (eq_w_pixels / 2)
                pos_y = center_y - (eq_settings.get('height', 200) / 2)

                app.log_message(f"[ë””ë²„ê·¸] EQ ìœ„ì¹˜: ({pos_x}, {pos_y}), ë„ˆë¹„: {eq_w_pixels}px")

                # ë¦¬ì‚¬ì´ì¦ˆ
                resized_vis_clip = vis_clip.resized(width=eq_w_pixels)
                app.log_message(f"[ë””ë²„ê·¸] ë¹„ì£¼ì–¼ë¼ì´ì € ë¦¬ì‚¬ì´ì¦ˆ ì™„ë£Œ: {resized_vis_clip.size}")

                # ìë§‰ í´ë¦½ ìƒì„±
                subtitle_clips = []
                subtitle_settings = job.get('subtitle_settings', {})
                subtitle_enabled = subtitle_settings.get('enabled', False)  # ê¸°ë³¸ê°’ False (OFF)
                app.log_message(f"[ë””ë²„ê·¸] ìë§‰ ì„¤ì •: {subtitle_settings}")
                app.log_message(f"[ë””ë²„ê·¸] ìë§‰ í™œì„±í™”: {subtitle_enabled}")

                if subtitle_enabled:
                    app.log_message(f"[ë””ë²„ê·¸] ìë§‰ ìƒì„± ì‹œì‘...")

                    # PIL ëª¨ë“ˆ import (Image ë³€ìˆ˜ ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•´ ë³„ì¹­ ì‚¬ìš©)
                    from PIL import Image as PILImage, ImageDraw, ImageFont

                    # ìë§‰ ì„¤ì • ì¶”ì¶œ
                    sub_font = subtitle_settings.get('font', 'Noto Sans KR')
                    sub_size = int(subtitle_settings.get('size', 24) * (img_h / 1080))  # í•´ìƒë„ì— ë§ê²Œ ìŠ¤ì¼€ì¼
                    sub_size = max(sub_size, 20)  # ìµœì†Œ í¬ê¸° ë³´ì¥
                    sub_color = subtitle_settings.get('color', '#ffffff')
                    sub_bg_color = subtitle_settings.get('bgColor', '#000000')
                    sub_bg_opacity = subtitle_settings.get('bgOpacity', 70) / 100.0
                    sub_bg_none = subtitle_settings.get('bgNone', False)
                    sub_x = subtitle_settings.get('x', 50) / 100.0  # í¼ì„¼íŠ¸ -> ë¹„ìœ¨
                    sub_y = subtitle_settings.get('y', 90) / 100.0

                    app.log_message(f"[ë””ë²„ê·¸] ìë§‰ í¬ê¸°: {sub_size}px, ìƒ‰ìƒ: {sub_color}, ìœ„ì¹˜: ({sub_x}, {sub_y})")

                    # í°íŠ¸ ë¡œë“œ (Windows ê¸°ë³¸ í•œê¸€ í°íŠ¸)
                    font_path = None
                    font_candidates = [
                        'C:/Windows/Fonts/malgun.ttf',      # ë§‘ì€ ê³ ë”•
                        'C:/Windows/Fonts/NanumGothic.ttf', # ë‚˜ëˆ”ê³ ë”•
                        'C:/Windows/Fonts/gulim.ttc',       # êµ´ë¦¼
                        '/usr/share/fonts/truetype/nanum/NanumGothic.ttf',  # Linux
                    ]
                    for fc in font_candidates:
                        if os.path.exists(fc):
                            font_path = fc
                            break

                    if font_path:
                        pil_font = ImageFont.truetype(font_path, sub_size)
                        app.log_message(f"[ë””ë²„ê·¸] í°íŠ¸ ë¡œë“œ: {font_path}")
                    else:
                        pil_font = ImageFont.load_default()
                        app.log_message(f"  âš ï¸ í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©")

                    # ê° í´ë¦½ë³„ ìë§‰ ìƒì„±
                    current_time = 0
                    for i, (clip, audio_seg) in enumerate(zip(clips, audio_segments)):
                        clip_duration = len(audio_seg) / 1000.0  # ms -> ì´ˆ
                        clip_text = clip['text']

                        try:
                            # í…ìŠ¤íŠ¸ í¬ê¸° ì¸¡ì •
                            temp_img = PILImage.new('RGBA', (1, 1))
                            temp_draw = ImageDraw.Draw(temp_img)
                            bbox = temp_draw.textbbox((0, 0), clip_text, font=pil_font)
                            text_w = bbox[2] - bbox[0]
                            text_h = bbox[3] - bbox[1]

                            # íŒ¨ë”© ì¶”ê°€
                            padding_x = 20
                            padding_y = 10
                            img_w_sub = text_w + padding_x * 2
                            img_h_sub = text_h + padding_y * 2

                            # ìë§‰ ì´ë¯¸ì§€ ìƒì„± (RGBA)
                            subtitle_img = PILImage.new('RGBA', (img_w_sub, img_h_sub), (0, 0, 0, 0))
                            draw = ImageDraw.Draw(subtitle_img)

                            # ë°°ê²½ ê·¸ë¦¬ê¸° (ë°˜íˆ¬ëª…)
                            if not sub_bg_none:
                                # ë°°ê²½ìƒ‰ íŒŒì‹±
                                bg_r = int(sub_bg_color[1:3], 16)
                                bg_g = int(sub_bg_color[3:5], 16)
                                bg_b = int(sub_bg_color[5:7], 16)
                                bg_a = int(255 * sub_bg_opacity)

                                # ë‘¥ê·¼ ì‚¬ê°í˜• ë°°ê²½
                                draw.rounded_rectangle(
                                    [(0, 0), (img_w_sub, img_h_sub)],
                                    radius=8,
                                    fill=(bg_r, bg_g, bg_b, bg_a)
                                )

                            # í…ìŠ¤íŠ¸ ìƒ‰ìƒ íŒŒì‹±
                            txt_r = int(sub_color[1:3], 16)
                            txt_g = int(sub_color[3:5], 16)
                            txt_b = int(sub_color[5:7], 16)

                            # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸° (ì™¸ê³½ì„  ìˆìœ¼ë©´ ë¨¼ì € ê·¸ë¦¼)
                            if sub_bg_none:
                                # ì™¸ê³½ì„  ê·¸ë¦¬ê¸°
                                for dx in [-2, -1, 0, 1, 2]:
                                    for dy in [-2, -1, 0, 1, 2]:
                                        if dx != 0 or dy != 0:
                                            draw.text((padding_x + dx, padding_y + dy), clip_text, font=pil_font, fill=(0, 0, 0, 255))

                            # ë©”ì¸ í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°
                            draw.text((padding_x, padding_y), clip_text, font=pil_font, fill=(txt_r, txt_g, txt_b, 255))

                            # PIL ì´ë¯¸ì§€ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜ (RGBA)
                            subtitle_array = np.array(subtitle_img)

                            # RGBì™€ ì•ŒíŒŒ ì±„ë„ ë¶„ë¦¬
                            rgb_array = subtitle_array[:, :, :3]
                            alpha_array = subtitle_array[:, :, 3] / 255.0  # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”

                            # ImageClip ìƒì„± (RGB)
                            txt_clip = ImageClip(rgb_array)

                            # ì•ŒíŒŒ ë§ˆìŠ¤í¬ ìƒì„±
                            mask_clip = ImageClip(alpha_array, is_mask=True)
                            txt_clip = txt_clip.with_mask(mask_clip)

                            # ìë§‰ ìœ„ì¹˜ ì„¤ì • (ì¤‘ì•™ ê¸°ì¤€ì )
                            txt_x = int(sub_x * img_w - img_w_sub / 2)
                            txt_y = int(sub_y * img_h - img_h_sub / 2)

                            # ì‹œê°„ ì„¤ì •
                            txt_clip = txt_clip.with_start(current_time).with_duration(clip_duration).with_position((txt_x, txt_y))
                            subtitle_clips.append(txt_clip)

                            if i == 0:
                                app.log_message(f"  âœ“ ìë§‰ 1 ìƒì„±: '{clip_text[:20]}...' ({img_w_sub}x{img_h_sub}px)")

                        except Exception as e:
                            app.log_message(f"  âš ï¸ ìë§‰ {i+1} ìƒì„± ì‹¤íŒ¨: {e}")
                            import traceback
                            app.log_message(f"  {traceback.format_exc()}")

                        current_time += clip_duration

                    app.log_message(f"[ë””ë²„ê·¸] ìë§‰ {len(subtitle_clips)}ê°œ ìƒì„± ì™„ë£Œ")

                # ìµœì¢… í•©ì„± (ë°°ê²½ + EQ + ìë§‰)
                eq_layer = resized_vis_clip.with_position((pos_x, pos_y))
                app.log_message(f"[ë””ë²„ê·¸] EQ ë ˆì´ì–´ ìƒì„±: position=({pos_x}, {pos_y}), size={eq_layer.size}, mask={eq_layer.mask is not None}")

                composite_layers = [final_bg, eq_layer]
                composite_layers.extend(subtitle_clips)
                app.log_message(f"[ë””ë²„ê·¸] í•©ì„± ë ˆì´ì–´ ìˆ˜: {len(composite_layers)} (ë°°ê²½ + EQ + ìë§‰ {len(subtitle_clips)}ê°œ)")

                final_clip = CompositeVideoClip(composite_layers, size=(img_w, img_h)).with_audio(audio_clip)

                # ìµœì¢… í•©ì„± ê²°ê³¼ í”„ë ˆì„ í…ŒìŠ¤íŠ¸
                final_test_frame = final_clip.get_frame(0.5)  # 0.5ì´ˆ ì§€ì  í”„ë ˆì„
                app.log_message(f"[ë””ë²„ê·¸] ìµœì¢… í•©ì„± í”„ë ˆì„ shape: {final_test_frame.shape}, dtype: {final_test_frame.dtype}")
                app.log_message(f"[ë””ë²„ê·¸] ìµœì¢… í´ë¦½ ìƒì„± ì™„ë£Œ: {final_clip.size}, {final_clip.duration}ì´ˆ")
                app.log_message(f"[ë””ë²„ê·¸] ì¶œë ¥ ê²½ë¡œ: {job['output_path']}")
            
                # FPSëŠ” jobì˜ eq_settingsì—ì„œ ê°€ì ¸ì˜¤ê¸°
                fps = job['eq_settings'].get('fps', 20)

                # GPU ì¸ì½”ë”© ì‹œë„ (ì—†ìœ¼ë©´ CPUë¡œ fallback)
                # í”„ë¦¬ë¯¸ì–´ í”„ë¡œ ìŠ¤íƒ€ì¼: VBR, ë†’ì€ ë¹„íŠ¸ë ˆì´íŠ¸, í•˜ë“œì›¨ì–´ ì¸ì½”ë”©
                codec = "libx264"
                preset = "superfast"  # CPU ì¸ì½”ë”© í”„ë¦¬ì…‹
                ffmpeg_params = ['-crf', '23']  # í’ˆì§ˆ ì„¤ì • (ê¸°ë³¸ê°’, ì¢‹ì€ í’ˆì§ˆ)

                # GPU ê°€ì† ê°ì§€ ë° ì„¤ì • (í”„ë¦¬ë¯¸ì–´ í”„ë¡œ ìŠ¤íƒ€ì¼ ìµœì í™”)
                gpu_detected = False
                gpu_type = None
                try:
                    result = subprocess.run(
                        ['ffmpeg', '-hide_banner', '-encoders'],
                        capture_output=True,
                        text=True,
                        timeout=2,
                        startupinfo=SUBPROCESS_STARTUP_INFO,
                        creationflags=SUBPROCESS_CREATION_FLAGS
                    )

                    # Intel QSV ìš°ì„  í™•ì¸ (í”„ë¦¬ë¯¸ì–´ í”„ë¡œê°€ ì‚¬ìš©í•˜ëŠ” ë°©ì‹)
                    # ëŒ€ë¶€ë¶„ì˜ PCì— Intel ë‚´ì¥ GPUê°€ ìˆìœ¼ë¯€ë¡œ í˜¸í™˜ì„±ì´ ë†’ìŒ
                    if 'h264_qsv' in result.stdout:
                        codec = "h264_qsv"
                        # í”„ë¦¬ë¯¸ì–´ í”„ë¡œ ìŠ¤íƒ€ì¼: VBR, 1íŒ¨ìŠ¤, ì•½ 19Mbps
                        ffmpeg_params = [
                            '-look_ahead', '1',           # lookahead í™œì„±í™” (í’ˆì§ˆ í–¥ìƒ)
                            '-global_quality', '23',      # í’ˆì§ˆ ë ˆë²¨ (ë‚®ì„ìˆ˜ë¡ ê³ í’ˆì§ˆ)
                            '-b:v', '15M',                # ëª©í‘œ ë¹„íŠ¸ë ˆì´íŠ¸ 15Mbps
                            '-maxrate', '20M',            # ìµœëŒ€ ë¹„íŠ¸ë ˆì´íŠ¸ 20Mbps
                            '-bufsize', '25M',            # ë²„í¼ í¬ê¸°
                        ]
                        gpu_detected = True
                        gpu_type = "Intel QSV"
                        app.log_message(f"  âœ… Intel Quick Sync ì¸ì½”ë” ê°ì§€ (í”„ë¦¬ë¯¸ì–´ í”„ë¡œ ìŠ¤íƒ€ì¼)")

                    # NVIDIA GPU í™•ì¸ (h264_nvenc)
                    elif 'h264_nvenc' in result.stdout:
                        codec = "h264_nvenc"
                        # NVIDIA ìµœì í™”: VBR, ë†’ì€ ë¹„íŠ¸ë ˆì´íŠ¸
                        ffmpeg_params = [
                            '-preset', 'p4',              # ê· í˜• ì¡íŒ í”„ë¦¬ì…‹
                            '-tune', 'hq',                # ê³ í’ˆì§ˆ íŠœë‹
                            '-rc', 'vbr',                 # VBR ëª¨ë“œ
                            '-cq', '23',                  # í’ˆì§ˆ ë ˆë²¨
                            '-b:v', '15M',                # ëª©í‘œ ë¹„íŠ¸ë ˆì´íŠ¸
                            '-maxrate', '20M',            # ìµœëŒ€ ë¹„íŠ¸ë ˆì´íŠ¸
                            '-bufsize', '25M',            # ë²„í¼ í¬ê¸°
                        ]
                        gpu_detected = True
                        gpu_type = "NVIDIA NVENC"
                        app.log_message(f"  âœ… NVIDIA NVENC ì¸ì½”ë” ê°ì§€")

                    # AMD GPU í™•ì¸ (h264_amf)
                    elif 'h264_amf' in result.stdout:
                        codec = "h264_amf"
                        ffmpeg_params = [
                            '-quality', 'balanced',       # ê· í˜• ëª¨ë“œ
                            '-rc', 'vbr_peak',            # VBR ëª¨ë“œ
                            '-b:v', '15M',
                            '-maxrate', '20M',
                        ]
                        gpu_detected = True
                        gpu_type = "AMD AMF"
                        app.log_message(f"  âœ… AMD AMF ì¸ì½”ë” ê°ì§€")

                    else:
                        # CPU ì¸ì½”ë”© (ìµœì í™”)
                        ffmpeg_params = [
                            '-crf', '23',                 # ì¢‹ì€ í’ˆì§ˆ
                            '-preset', 'fast',            # ë¹ ë¥¸ í”„ë¦¬ì…‹ (superfastë³´ë‹¤ í’ˆì§ˆ ì¢‹ìŒ)
                        ]
                        app.log_message(f"  â„¹ï¸ CPU ì¸ì½”ë”© ì‚¬ìš© (í•˜ë“œì›¨ì–´ ì¸ì½”ë” ì—†ìŒ)")

                except Exception as e:
                    app.log_message(f"  â„¹ï¸ GPU ê°ì§€ ì‹¤íŒ¨, CPU ì¸ì½”ë”© ì‚¬ìš©: {e}")

                # ì˜¤ë””ì˜¤ ì„¤ì • (í”„ë¦¬ë¯¸ì–´ í”„ë¡œ ìŠ¤íƒ€ì¼: AAC 320kbps)
                audio_bitrate = "320k"

                # ì§„í–‰ë¥  í‘œì‹œë¥¼ ìœ„í•œ ì™„ë²½í•œ ì»¤ìŠ¤í…€ logger
                class ProgressLogger:
                    def __init__(self, app, total_frames, fps):
                        self.app = app
                        self.total_frames = total_frames
                        self.fps = fps
                        self.last_log_time = time.time()
                        self.start_time = time.time()
                        self.current_frame = 0
                        self.bars = {}  # ì—¬ëŸ¬ í”„ë¡œê·¸ë ˆìŠ¤ ë°” ì¶”ì 

                    def __call__(self, message=None, **kwargs):
                        # MoviePyëŠ” 't' (í˜„ì¬ ì‹œê°„, ì´ˆ ë‹¨ìœ„)ë¥¼ ì „ë‹¬í•¨
                        current_time = kwargs.get('t', 0)
                        self.current_frame = int(current_time * self.fps)

                        # 5ì´ˆë§ˆë‹¤ í•œ ë²ˆì”© ì§„í–‰ë¥  ë¡œê·¸
                        now = time.time()
                        if now - self.last_log_time >= 5:
                            if self.total_frames > 0:
                                progress = min(100, (self.current_frame / self.total_frames) * 100)
                                elapsed = int(now - self.start_time)
                                elapsed_str = f"{elapsed // 60}ë¶„ {elapsed % 60}ì´ˆ"
                                self.app.log_message(f"  ğŸ¬ ì¸ì½”ë”© ì§„í–‰ ì¤‘: {progress:.1f}% (ê²½ê³¼: {elapsed_str})")

                            self.last_log_time = now

                    def iter_bar(self, chunk=None, **kwargs):
                        """MoviePyì˜ iter_bar ë©”ì„œë“œ - ì˜¤ë””ì˜¤/ë¹„ë””ì˜¤ ì²­í¬ ë°˜ë³µ ì²˜ë¦¬"""
                        if chunk is not None:
                            # ì²­í¬ë¥¼ ìˆœíšŒí•˜ë©´ì„œ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                            total = len(chunk) if hasattr(chunk, '__len__') else None
                            for i, item in enumerate(chunk):
                                # ì£¼ê¸°ì ìœ¼ë¡œ ì§„í–‰ë¥  ì²´í¬
                                if total and i % max(1, total // 20) == 0:  # 5% ë‹¨ìœ„ë¡œ ì²´í¬
                                    now = time.time()
                                    if now - self.last_log_time >= 5:
                                        progress = (i / total) * 100
                                        elapsed = int(now - self.start_time)
                                        self.app.log_message(f"  ğŸµ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì¤‘: {progress:.1f}%")
                                        self.last_log_time = now
                                yield item
                        else:
                            return iter([])

                    def bars(self, name=None):
                        """MoviePyì˜ bars ë©”ì„œë“œ - í”„ë¡œê·¸ë ˆìŠ¤ ë°” ê°ì²´ ë°˜í™˜"""
                        if name not in self.bars:
                            self.bars[name] = self
                        return self.bars[name]

                    def add(self, n=1):
                        """í”„ë¡œê·¸ë ˆìŠ¤ ë°” ì—…ë°ì´íŠ¸"""
                        pass

                    def update(self, n=1):
                        """í”„ë¡œê·¸ë ˆìŠ¤ ë°” ì—…ë°ì´íŠ¸"""
                        pass

                    def close(self):
                        """í”„ë¡œê·¸ë ˆìŠ¤ ë°” ì¢…ë£Œ"""
                        pass

                # ì´ í”„ë ˆì„ ìˆ˜ ê³„ì‚°
                total_frames = int(final_clip.duration * fps)
                progress_logger = ProgressLogger(app, total_frames, fps)

                # ì¸ì½”ë”© ì‹œì‘ ë©”ì‹œì§€
                duration_min = int(final_clip.duration // 60)
                duration_sec = int(final_clip.duration % 60)
                app.log_message(f"  ğŸ“¹ ì˜ìƒ ì¸ì½”ë”© ì‹œì‘ (ê¸¸ì´: {duration_min}ë¶„ {duration_sec}ì´ˆ, ì´ {total_frames:,}í”„ë ˆì„)")

                # ì˜ìƒ íŒŒì¼ ì‘ì„± (GPU ì‹¤íŒ¨ ì‹œ CPUë¡œ ìë™ ì „í™˜)
                encoding_success = False

                # ì§„í–‰ë¥  ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ ì¤€ë¹„
                stop_monitor = threading.Event()
                monitor_thread = threading.Thread(
                    target=_monitor_encoding_progress,
                    args=(job['output_path'], app, stop_monitor, final_clip.duration),
                    daemon=True
                )

                try:
                    # ì§„í–‰ë¥  ëª¨ë‹ˆí„°ë§ ì‹œì‘
                    monitor_thread.start()

                    # GPU ì¸ì½”ë”ëŠ” preset ì˜µì…˜ì„ ì§€ì›í•˜ì§€ ì•ŠìŒ (ffmpeg_paramsì— í¬í•¨ë¨)
                    write_params = {
                        'codec': codec,
                        'audio_codec': "aac",
                        'audio_bitrate': audio_bitrate,  # í”„ë¦¬ë¯¸ì–´ ìŠ¤íƒ€ì¼: 320kbps
                        'threads': os.cpu_count() or 4,
                        'fps': fps,
                        'logger': None,
                        'ffmpeg_params': ffmpeg_params
                    }

                    # CPU ì¸ì½”ë”©ì¼ ë•Œë§Œ preset ì¶”ê°€
                    if not gpu_detected:
                        write_params['preset'] = preset

                    final_clip.write_videofile(job['output_path'], **write_params)

                    # ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
                    stop_monitor.set()

                    # íŒŒì¼ í¬ê¸° ê²€ì¦ (0 bytesë©´ ì‹¤íŒ¨)
                    if os.path.exists(job['output_path']):
                        file_size = os.path.getsize(job['output_path'])
                        if file_size == 0:
                            raise RuntimeError("ì¸ì½”ë”©ëœ íŒŒì¼ í¬ê¸°ê°€ 0 bytesì…ë‹ˆë‹¤. GPU í•˜ë“œì›¨ì–´ ë¯¸ì§€ì›ìœ¼ë¡œ ì¶”ì •ë©ë‹ˆë‹¤.")
                        encoding_success = True
                        if gpu_detected:
                            file_size_mb = file_size / (1024 * 1024)
                            app.log_message(f"  âœ“ {gpu_type} ì¸ì½”ë”© ì„±ê³µ ({file_size_mb:.1f} MB)")
                    else:
                        raise RuntimeError("ì¶œë ¥ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                except Exception as gpu_error:
                    if gpu_detected:
                        # GPU ì¸ì½”ë”© ì‹¤íŒ¨, CPUë¡œ ì¬ì‹œë„
                        error_msg = str(gpu_error)[:150]
                        app.log_message(f"  âš ï¸ {gpu_type} ì¸ì½”ë”© ì‹¤íŒ¨, CPUë¡œ ì¬ì‹œë„ ì¤‘...")
                        app.log_message(f"     ì˜¤ë¥˜: {error_msg}")

                        # CPU ì„¤ì •ìœ¼ë¡œ ë³€ê²½
                        codec = "libx264"
                        preset = "fast"
                        ffmpeg_params = ['-crf', '23']

                        # ì´ì „ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
                        stop_monitor.set()

                        # CPU ì¬ì‹œë„ ì‹œì‘ ë©”ì‹œì§€
                        app.log_message(f"  ğŸ“¹ CPU ì¸ì½”ë”© ì‹œì‘ (ê¸¸ì´: {duration_min}ë¶„ {duration_sec}ì´ˆ, ì´ {total_frames:,}í”„ë ˆì„)")

                        # ìƒˆë¡œìš´ ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ ì‹œì‘
                        stop_monitor_cpu = threading.Event()
                        monitor_thread_cpu = threading.Thread(
                            target=_monitor_encoding_progress,
                            args=(job['output_path'], app, stop_monitor_cpu, final_clip.duration),
                            daemon=True
                        )
                        monitor_thread_cpu.start()

                        # CPUë¡œ ì¬ì‹œë„
                        final_clip.write_videofile(
                            job['output_path'],
                            codec=codec,
                            preset=preset,
                            audio_codec="aac",
                            audio_bitrate=audio_bitrate,
                            threads=os.cpu_count() or 4,
                            fps=fps,
                            logger=None,
                            ffmpeg_params=ffmpeg_params
                        )

                        # ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
                        stop_monitor_cpu.set()

                        # íŒŒì¼ í¬ê¸° ê²€ì¦ (0 bytesë©´ ì‹¤íŒ¨)
                        if os.path.exists(job['output_path']):
                            file_size = os.path.getsize(job['output_path'])
                            if file_size == 0:
                                raise RuntimeError("CPU ì¸ì½”ë”© í›„ì—ë„ íŒŒì¼ í¬ê¸°ê°€ 0 bytesì…ë‹ˆë‹¤.")
                            encoding_success = True
                            file_size_mb = file_size / (1024 * 1024)
                            app.log_message(f"  âœ“ CPU ì¸ì½”ë”© ì„±ê³µ ({file_size_mb:.1f} MB)")
                        else:
                            raise RuntimeError("CPU ì¸ì½”ë”© í›„ ì¶œë ¥ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    else:
                        # CPU ì¸ì½”ë”©ë„ ì‹¤íŒ¨
                        raise

        else:
            # EQ ì—†ì´ ë°°ê²½ ì´ë¯¸ì§€ + ì˜¤ë””ì˜¤ë§Œ ê²°í•©
            with AudioFileClip(audio_path) as audio_clip, \
                 ImageClip(job['image_path'], duration=audio_clip.duration) as bg_clip_orig:

                # ë°°ê²½ ì´ë¯¸ì§€ë¥¼ ì¶œë ¥ í•´ìƒë„ì— ë§ê²Œ ë¦¬ì‚¬ì´ì¦ˆ
                if bg_clip_orig.size != (img_w, img_h):
                    bg_clip = bg_clip_orig.resized((img_w, img_h))
                    app.log_message(f"[ë””ë²„ê·¸] ë°°ê²½ ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ (EQ ì—†ìŒ): {bg_clip_orig.size} -> {bg_clip.size}")
                else:
                    bg_clip = bg_clip_orig

                app.log_message(f"[ë””ë²„ê·¸] í´ë¦½ ë¡œë“œ ì™„ë£Œ (EQ ì—†ìŒ):")
                app.log_message(f"  - audio_clip ê¸¸ì´: {audio_clip.duration}ì´ˆ")
                app.log_message(f"  - bg_clip í¬ê¸°: {bg_clip.size}, ê¸¸ì´: {bg_clip.duration}ì´ˆ")

                final_bg = bg_clip
                # ë°ê¸° ì¡°ì ˆ
                brightness_val = 100.0
                if not is_batch:
                    if hasattr(app, 'video_maker_tab') and hasattr(app.video_maker_tab, 'brightness_var'):
                        brightness_val = app.video_maker_tab.brightness_var.get()
                    elif 'eq_settings' in job and 'brightness' in job['eq_settings']:
                        brightness_val = float(job['eq_settings']['brightness'])

                    if brightness_val != 100.0:
                        base_brightness = brightness_val / 100.0
                        oscillation = 0.1
                        period = 10
                        def brightness_func(t): return base_brightness + oscillation * math.sin(2 * math.pi * t / period)
                        final_bg = final_bg.fl(lambda gf, t: (gf(t) * brightness_func(t)).clip(0,255).astype('uint8'))

                final_clip = final_bg.with_audio(audio_clip)
                app.log_message(f"[ë””ë²„ê·¸] ìµœì¢… í´ë¦½ ìƒì„± ì™„ë£Œ (EQ ì—†ìŒ): {final_clip.size}, {final_clip.duration}ì´ˆ")
                app.log_message(f"[ë””ë²„ê·¸] ì¶œë ¥ ê²½ë¡œ: {job['output_path']}")

                # FPS
                fps = eq_settings.get('fps', 30)

                # GPU ì¸ì½”ë”©
                codec = "libx264"
                preset = "superfast"
                ffmpeg_params = ['-crf', '23']
                gpu_detected = False

                try:
                    result = subprocess.run(
                        ['ffmpeg', '-hide_banner', '-encoders'],
                        capture_output=True,
                        text=True,
                        timeout=2,
                        startupinfo=SUBPROCESS_STARTUP_INFO,
                        creationflags=SUBPROCESS_CREATION_FLAGS
                    )

                    if 'h264_qsv' in result.stdout:
                        codec = "h264_qsv"
                        ffmpeg_params = ['-look_ahead', '1', '-global_quality', '23', '-b:v', '15M', '-maxrate', '20M', '-bufsize', '25M']
                        gpu_detected = True
                        app.log_message(f"  âœ… Intel Quick Sync ì¸ì½”ë” ê°ì§€")
                    elif 'h264_nvenc' in result.stdout:
                        codec = "h264_nvenc"
                        ffmpeg_params = ['-preset', 'p4', '-tune', 'hq', '-rc', 'vbr', '-cq', '23', '-b:v', '15M', '-maxrate', '20M', '-bufsize', '25M']
                        gpu_detected = True
                        app.log_message(f"  âœ… NVIDIA NVENC ì¸ì½”ë” ê°ì§€")
                except:
                    pass

                # ì¸ì½”ë”©
                audio_bitrate = "320k"
                write_params = {
                    'codec': codec,
                    'audio_codec': "aac",
                    'audio_bitrate': audio_bitrate,
                    'threads': os.cpu_count() or 4,
                    'fps': fps,
                    'logger': None,
                    'ffmpeg_params': ffmpeg_params
                }

                if not gpu_detected:
                    write_params['preset'] = preset

                app.log_message(f"  ğŸ“¹ ì˜ìƒ ì¸ì½”ë”© ì‹œì‘ (EQ ì—†ìŒ, codec: {codec})")
                final_clip.write_videofile(job['output_path'], **write_params)

                file_size = os.path.getsize(job['output_path'])
                file_size_mb = file_size / (1024 * 1024)
                app.log_message(f"  âœ“ ì¸ì½”ë”© ì„±ê³µ ({file_size_mb:.1f} MB)")

        # SRT ìë§‰ íŒŒì¼ ìƒì„± (ë°°ì¹˜ ëª¨ë“œ í¬í•¨)
        if not app.cancel_event.is_set() and clips and audio_segments:
            try:
                srt_path = job['output_path'].replace('.mp4', '.srt')
                app.log_message(f"\nğŸ“ SRT ìë§‰ íŒŒì¼ ìƒì„± ì¤‘...")
                generate_srt_from_clips(clips, audio_segments, srt_path, app=app)
            except Exception as e:
                app.log_message(f"âš ï¸ SRT ìƒì„± ì‹¤íŒ¨ (ì˜ìƒì€ ì •ìƒ ìƒì„±ë¨): {e}")

        # í”„ë¦¬ë¯¸ì–´ í”„ë¡œ í”„ë¡œì íŠ¸ íŒŒì¼ ìƒì„± (ì˜µì…˜)
        if not app.cancel_event.is_set() and job.get('create_premiere_project', False):
            try:
                app.log_message(f"\nğŸ“ í”„ë¦¬ë¯¸ì–´ í”„ë¡œ í”„ë¡œì íŠ¸ ìƒì„± ì¤‘...")
                import premiere_project

                prproj_path = premiere_project.create_premiere_project(
                    video_path=job['output_path'],
                    background_image=job['image_path'],
                    eq_video=vis_path,
                    width=img_w,
                    height=img_h,
                    fps=fps,
                    audio_path=audio_path
                )

                app.log_message(f"âœ… í”„ë¦¬ë¯¸ì–´ í”„ë¡œì íŠ¸ ìƒì„± ì™„ë£Œ: {os.path.basename(prproj_path)}")
                app.log_message(f"   ì´ì œ í”„ë¦¬ë¯¸ì–´ í”„ë¡œì—ì„œ .prproj íŒŒì¼ì„ ì—´ ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
            except Exception as e:
                app.log_message(f"âš ï¸ í”„ë¦¬ë¯¸ì–´ í”„ë¡œì íŠ¸ ìƒì„± ì‹¤íŒ¨ (ì˜ìƒì€ ì •ìƒ ìƒì„±ë¨): {e}")

        if not app.cancel_event.is_set() and not is_batch:
            # Tkinter ë²„ì „ë§Œ CompletionDialog í‘œì‹œ
            if hasattr(app, 'root') and app.root is not None:
                from ui_dialogs import CompletionDialog
                app.root.after(0, lambda: CompletionDialog(app.root, "ì œì‘ ì™„ë£Œ", job['output_path']))

        # ë°°ì¹˜ ëª¨ë“œì¼ ë•ŒëŠ” dict ë°˜í™˜
        if is_batch:
            return {
                'success': True,
                'output_path': job.get('output_path', ''),
                'fileName': job.get('fileName', 'output')
            }
        else:
            return True

    except Exception as e:
        import traceback
        app.log_message(f"ì˜ìƒ ì œì‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\n{traceback.format_exc()}")

        # ë°°ì¹˜ ëª¨ë“œì¼ ë•ŒëŠ” dict ë°˜í™˜
        if is_batch:
            return {
                'success': False,
                'error': str(e)
            }
        else:
            return False
    finally:
        import shutil
        for f in temp_files:
            if os.path.exists(f):
                try:
                    if os.path.isdir(f):
                        shutil.rmtree(f)
                    else:
                        os.remove(f)
                except Exception as e:
                    app.log_message(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")

def _execute_single_video_job_transparent(job, app, cancel_event):
    """íˆ¬ëª… ë°°ê²½ EQ ì˜ìƒ ìƒì„± (ë°°ê²½ ì´ë¯¸ì§€ ì—†ì´ EQë§Œ ë Œë”ë§)"""
    temp_files = []
    try:
        app.log_message(f"\n[ë””ë²„ê·¸] íˆ¬ëª… EQ ì˜ìƒ ìƒì„± ì‹œì‘")
        app.log_message(f"  - eq_settings: {job.get('eq_settings', {})}")

        app.update_progress("ì˜¤ë””ì˜¤ ìƒì„± ì‹œì‘...", 5)
        combined_audio = AudioSegment.empty()
        audio_segments = []  # ê° í´ë¦½ë³„ ì˜¤ë””ì˜¤ ì €ì¥ (SRT ìƒì„±ìš©)
        clips = job['clips']

        for i, clip in enumerate(clips):
            if cancel_event.is_set():
                return {'status': 'cancelled'}

            char = clip['character']
            text_preview = clip['text'][:50] + "..." if len(clip['text']) > 50 else clip['text']
            app.log_message(f"\n[í´ë¦½ {i+1}/{len(clips)}] '{char}' ì²˜ë¦¬ ì¤‘...")
            app.log_message(f"  í…ìŠ¤íŠ¸: {text_preview}")

            app.update_progress(f"ìŒì„± ìƒì„± ì¤‘ ({i+1}/{len(clips)})...", 5 + (i/len(clips)*35))

            # ìºë¦­í„°ë³„ ìŒì„± ì„¤ì • ì‚¬ìš©
            if char in job['narration_settings']:
                w = job['narration_settings'][char]
                # API ìŒì„± ì´ë¦„ ì¶”ì¶œ
                api_voice = w.get('voice', '')
                rate = float(w.get('speed', 1.0))
                pitch = float(w.get('pitch', 0.0))
                volume_gain = float(w.get('volumeGain', 0))
            else:
                app.log_message(f"  ê²½ê³ : '{char}' ìŒì„± ì„¤ì • ì—†ìŒ, ê¸°ë³¸ ì„¤ì • ì‚¬ìš©")
                # ê¸°ë³¸ ìŒì„± ì„¤ì • (í•œêµ­ì–´ Standard A)
                api_voice = 'ko-KR-Standard-A'
                rate = 1.0
                pitch = 0.0
                volume_gain = 0

            audio_bytes = synthesize_tts_bytes(
                job['api_key_profile'],
                clip['text'],
                api_voice,
                rate,
                pitch,
                volume_gain,
                clip.get('is_ssml', False),
                app=app
            )
            audio_seg = AudioSegment.from_mp3(io.BytesIO(audio_bytes))
            audio_segments.append(audio_seg)  # SRT ìƒì„±ìš© ì €ì¥
            combined_audio += audio_seg
            app.log_message(f"  âœ“ ì™„ë£Œ!")

        if cancel_event.is_set():
            return {'status': 'cancelled'}

        audio_path = os.path.join(TEMP_DIR, f"temp_audio_{job.get('id', uuid.uuid4())}.mp3")
        temp_files.append(audio_path)
        combined_audio.export(audio_path, format="mp3")
        app.log_message(f"\n[ë””ë²„ê·¸] ì˜¤ë””ì˜¤ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {audio_path}")

        app.update_progress("ë¹„ì£¼ì–¼ë¼ì´ì € ë Œë”ë§...", 40)
        app.log_message(f"[ë””ë²„ê·¸] ë¹„ì£¼ì–¼ë¼ì´ì € ë Œë”ë§ ì‹œì‘...")
        vis_path = render_visualizer_video(app, audio_path, job, is_batch=False)
        if not vis_path:
            app.log_message(f"[ì˜¤ë¥˜] ë¹„ì£¼ì–¼ë¼ì´ì € ë Œë”ë§ ì‹¤íŒ¨")
            return {'status': 'error', 'error': 'ë¹„ì£¼ì–¼ë¼ì´ì € ë Œë”ë§ ì‹¤íŒ¨'}
        app.log_message(f"[ë””ë²„ê·¸] ë¹„ì£¼ì–¼ë¼ì´ì € ë Œë”ë§ ì™„ë£Œ: {vis_path}")
        temp_files.append(vis_path)

        if cancel_event.is_set():
            return {'status': 'cancelled'}

        app.update_progress("íˆ¬ëª… ë°°ê²½ ì˜ìƒ ìƒì„± ì¤‘...", 85)
        app.log_message(f"[ë””ë²„ê·¸] íˆ¬ëª… ë°°ê²½ ì˜ìƒ ìƒì„± ì‹œì‘...")

        # RoyStudio ë°©ì‹: ì´ë¯¸ ìƒì„±ëœ MOV íŒŒì¼ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì´ë¯¸ íˆ¬ëª… ë°°ê²½)
        # ì˜¤ë””ì˜¤ë§Œ ì¶”ê°€í•˜ë©´ ë¨
        with VideoFileClip(vis_path, has_mask=True) as vis_clip, \
             AudioFileClip(audio_path) as audio_clip:

            app.log_message(f"[ë””ë²„ê·¸] í´ë¦½ ë¡œë“œ ì™„ë£Œ:")
            app.log_message(f"  - vis_clip í¬ê¸°: {vis_clip.size}")
            app.log_message(f"  - audio_clip ê¸¸ì´: {audio_clip.duration}ì´ˆ")

            # EQ í´ë¦½ì— ì˜¤ë””ì˜¤ ì¶”ê°€
            final_clip = vis_clip.with_audio(audio_clip)
            app.log_message(f"[ë””ë²„ê·¸] ìµœì¢… í´ë¦½ ìƒì„± ì™„ë£Œ: {final_clip.size}, {final_clip.duration}ì´ˆ")
            app.log_message(f"[ë””ë²„ê·¸] ì¶œë ¥ ê²½ë¡œ: {job['output_path']}")

            # MOV í˜•ì‹ìœ¼ë¡œ ì €ì¥ (íˆ¬ëª… ë°°ê²½ ì§€ì›)
            fps = job['eq_settings'].get('fps', 20)
            final_clip.write_videofile(
                job['output_path'],
                codec="qtrle",  # QuickTime Animation codec (íˆ¬ëª… ë°°ê²½ ì§€ì›)
                fps=fps,
                ffmpeg_params=['-pix_fmt', 'argb'],  # íˆ¬ëª… ë°°ê²½ì„ ìœ„í•œ ARGB í”½ì…€ í¬ë§·
                threads=(os.cpu_count() or 1),
                logger=None
            )

        # SRT ìë§‰ íŒŒì¼ ìƒì„±
        if not cancel_event.is_set() and clips and audio_segments:
            try:
                srt_path = job['output_path'].replace('.mov', '.srt')
                app.log_message(f"\nğŸ“ SRT ìë§‰ íŒŒì¼ ìƒì„± ì¤‘...")
                generate_srt_from_clips(clips, audio_segments, srt_path, app=app)
            except Exception as e:
                app.log_message(f"âš ï¸ SRT ìƒì„± ì‹¤íŒ¨ (ì˜ìƒì€ ì •ìƒ ìƒì„±ë¨): {e}")

        app.log_message(f"âœ… íˆ¬ëª… EQ ì˜ìƒ ìƒì„± ì™„ë£Œ!")
        return {'status': 'success'}

    except Exception as e:
        import traceback
        error_msg = f"{str(e)}\n{traceback.format_exc()}"
        app.log_message(f"íˆ¬ëª… EQ ì˜ìƒ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error_msg}")
        return {'status': 'error', 'error': str(e)}
    finally:
        import shutil
        for f in temp_files:
            if os.path.exists(f):
                try:
                    if os.path.isdir(f):
                        shutil.rmtree(f)
                    else:
                        os.remove(f)
                except Exception as e:
                    app.log_message(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")